{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "QA_SR_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5lWKRGx8MFi",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #3\n",
        "\n",
        "\n",
        "Natural Language Processing / Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAbZbytE8Gf1",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "- **Assignment Due:** 1398/11/7 23:59\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. So, please mention his/her name if you have a team-mate.\n",
        "- Finding any sort of copying will zero down that assignment grade.\n",
        "- When your solution is ready to submit, don't forget to set the name of this notebook like  \"Name_StudentID.ipynb\".\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You can also ask your questions on the telegram group.\n",
        "- You must run this notebook on Google Colab platform; there are some dependencies to Google Colab VM for some of the libraries.\n",
        "\n",
        "<br/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbN-rpH2EAvG",
        "colab_type": "text"
      },
      "source": [
        "# 1. Question Answering Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJxhbj0wTG0i",
        "colab_type": "text"
      },
      "source": [
        "Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the Question Answering systems or completing a task, and models that can learn and describe long term dependencies in sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGGHbCHGrKi7",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Question Answering System with the use of Memory Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp76Jblf2yqH",
        "colab_type": "text"
      },
      "source": [
        "In this task we want to work on a simple presented QA system. The continuity of the model we present here means that it can be trained end to-end from input-output pairs, and so is applicable to more tasks, i.e. tasks where such supervision is not available, such as in language modeling or realistically supervised question answering tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qllnZMzyrKi_",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2ov8FZrKjC",
        "colab_type": "code",
        "outputId": "9b09a05b-64d2-4afb-8d41-58ec9234b572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
        "from keras.layers import add, dot, concatenate\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "\n",
        "from functools import reduce\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259V3MeC9G55",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JqUZuEo8G_p",
        "colab_type": "text"
      },
      "source": [
        "We will use a dataset consists of questions where a previously given single supporting fact, potentially amongst a set of other irrelevant facts, provides the answer. We first test one of the simplest cases of this, by asking for the location of a person, e.g. “$Mary$ $travelled$ $to$ $the$ $office.$ $Where$ $is$ $Mary?$”. It can be considered the\n",
        "simplest case of some real world QA datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsvVnfvfGnib",
        "colab_type": "text"
      },
      "source": [
        "About the dataset: https://research.fb.com/downloads/babi/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OlahJ1OrKjd",
        "colab_type": "text"
      },
      "source": [
        "Lets download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWvxAMh0rKje",
        "colab_type": "code",
        "outputId": "d62c88a6-0c51-41e3-aab1-6a702d464190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "try:\n",
        "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
        "except:\n",
        "    print('Error downloading dataset, please download it manually:\\n'\n",
        "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
        "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
        "    raise\n",
        "tar = tarfile.open(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
            "11747328/11745123 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u_xDAfovXf7",
        "colab_type": "text"
      },
      "source": [
        "Our model takes a discrete set of inputs $x_{1}, ..., x_{n}$ that are to be stored in the memory, a query $q$, and outputs an answer $a$. Each of the $x_{i}$, $q$, and $a$ contains symbols coming from a dictionary with $V$ words. The model writes all $x$ to the memory up to a fixed buffer size, and then finds a continuous representation for the $x$ and $q$. The continuous representation is then processed via multiple hops to\n",
        "output $a$. This allows backpropagation of the error signal through multiple memory accesses back to the input during training. The overall model is shown in the next figure. During training, all three embedding matrices $A, B$ and $C$, as well as $W$ are jointly learned by minimizing a standard cross-entropy loss between $aˆ$ and the true\n",
        "label $a$. Training is performed using stochastic gradient descent.\n",
        "\n",
        "\n",
        "Delve more deeply into the details: https://arxiv.org/pdf/1503.08895.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-7x0t7huYPh",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://cdn-21.anonfile.com/T9j89eK4n4/90735f05-1577960396/outview%20of%20model.jpg\" width=\"800\"/>   \n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSWkTNXKrKjK",
        "colab_type": "text"
      },
      "source": [
        "Write some helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCBfH4qrrKjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sent):\n",
        "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WLJmevS-Dy6",
        "colab_type": "text"
      },
      "source": [
        "According to the dataset (bAbi tasks), we need to prepare the data for training the model. With the next function we parse the dataset and manufactore it in desired way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzksSX8drKjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_stories(lines, only_supporting=False):\n",
        "    '''Parse stories provided in the bAbi tasks format\n",
        "    If only_supporting is true, only the sentences\n",
        "    that support the answer are kept.\n",
        "    '''\n",
        "    data = []\n",
        "    story = []\n",
        "    for line in lines:\n",
        "        line = line.decode('utf-8').strip()\n",
        "        nid, line = line.split(' ', 1)\n",
        "        nid = int(nid)\n",
        "        if nid == 1:\n",
        "            story = []\n",
        "        if '\\t' in line:\n",
        "            q, a, supporting = line.split('\\t')\n",
        "            q = tokenize(q)\n",
        "            substory = None\n",
        "            if only_supporting:\n",
        "                # Only select the related substory\n",
        "                supporting = map(int, supporting.split())\n",
        "                substory = [story[i - 1] for i in supporting]\n",
        "            else:\n",
        "                # Provide all the substories\n",
        "                substory = [x for x in story if x]\n",
        "            data.append((substory, q, a))\n",
        "            story.append('')\n",
        "        else:\n",
        "            sent = tokenize(line)\n",
        "            story.append(sent)\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwkiTJYRJvPe",
        "colab_type": "text"
      },
      "source": [
        "Now we need to take proper structure of the data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBc_rzF-rKjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stories(f, only_supporting=False, max_length=None):\n",
        "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FivvTCc8J-GW",
        "colab_type": "text"
      },
      "source": [
        "Here we need to make the vectors of stories, questions and answers. its too easy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzEnmDN_rKjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
        "  inputs_train = []\n",
        "  queries_train = []\n",
        "  answers_train = []\n",
        "\n",
        "  for story, query, answer in data:\n",
        "    inputs_train.append([word_idx[token] for token in story])\n",
        "    queries_train.append([word_idx[token] for token in query])\n",
        "    oh = np.zeros(len(word_idx) + 1)\n",
        "    oh[word_idx[answer]] = 1\n",
        "    answers_train.append(oh)\n",
        "\n",
        "    \n",
        "  inputs_train = pad_sequences(inputs_train, maxlen=story_maxlen)\n",
        "  queries_train = pad_sequences(queries_train, maxlen=query_maxlen)\n",
        "\n",
        "  return np.array(inputs_train),np.array(queries_train), np.array(answers_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B77myra2ZJYK",
        "colab_type": "text"
      },
      "source": [
        "Its time to extract stories from the dataset, then pass them to the defined functions for parsing and make it usable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akcwoo3frKjj",
        "colab_type": "code",
        "outputId": "09701b7d-50e4-4146-a354-d004c5eda1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "challenges = {\n",
        "    # QA1 with 10,000 samples\n",
        "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
        "    # QA2 with 10,000 samples\n",
        "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
        "}\n",
        "challenge_type = 'single_supporting_fact_10k'\n",
        "challenge = challenges[challenge_type]\n",
        "\n",
        "print('Extracting stories for the challenge:', challenge_type)\n",
        "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
        "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting stories for the challenge: single_supporting_fact_10k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyiMaVRirKjm",
        "colab_type": "code",
        "outputId": "b4fa13f2-f158-4785-eeda-25c7e5728b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_stories), len(test_stories)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpXH9iYQrKjq",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Check our helper functions and prepare the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29110FTDrKjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "for story, q, answer in train_stories + test_stories:\n",
        "    vocab |= set(story + q + [answer])\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# Reserve 0 for masking via pad_sequences\n",
        "vocab_size = len(vocab) + 1\n",
        "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
        "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PopB46hnrKju",
        "colab_type": "code",
        "outputId": "c8abb5f6-faba-41dc-97dd-5a5c13a5a950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "story_maxlen, query_maxlen"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-G22KfxrKjy",
        "colab_type": "code",
        "outputId": "eb12d319-e603-4996-ffc4-8e5737e59c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "print('-')\n",
        "print('Vocab size:', vocab_size, 'unique words')\n",
        "print('Story max length:', story_maxlen, 'words')\n",
        "print('Query max length:', query_maxlen, 'words')\n",
        "print('Number of training stories:', len(train_stories))\n",
        "print('Number of test stories:', len(test_stories))\n",
        "print('-')\n",
        "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
        "print(train_stories[0])\n",
        "print('-')\n",
        "print('Vectorizing the word sequences...')\n",
        "\n",
        "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
        "idx_word = dict((i+1, c) for i,c in enumerate(vocab))\n",
        "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
        "                                                               word_idx,\n",
        "                                                               story_maxlen,\n",
        "                                                               query_maxlen)\n",
        "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
        "                                                            word_idx,\n",
        "                                                            story_maxlen,\n",
        "                                                            query_maxlen)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Vocab size: 22 unique words\n",
            "Story max length: 68 words\n",
            "Query max length: 4 words\n",
            "Number of training stories: 10000\n",
            "Number of test stories: 1000\n",
            "-\n",
            "Here's what a \"story\" tuple looks like (input, query, answer):\n",
            "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
            "-\n",
            "Vectorizing the word sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDzEiO3I-V7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMnIXlTBrKj1",
        "colab_type": "code",
        "outputId": "bd9c9aee-ff79-47ec-f768-081c7296febb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs_train.shape, queries_train.shape, answers_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 68), (10000, 4), (10000, 22))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4lNOEBkrKj5",
        "colab_type": "code",
        "outputId": "3cba0c5a-2f54-492f-974b-614276d54534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "print('-')\n",
        "print('inputs: integer tensor of shape (samples, max_length)')\n",
        "print('inputs_train shape:', inputs_train.shape)\n",
        "print('inputs_test shape:', inputs_test.shape)\n",
        "print('-')\n",
        "print('queries: integer tensor of shape (samples, max_length)')\n",
        "print('queries_train shape:', queries_train.shape)\n",
        "print('queries_test shape:', queries_test.shape)\n",
        "print('-')\n",
        "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
        "print('answers_train shape:', answers_train.shape)\n",
        "print('answers_test shape:', answers_test.shape)\n",
        "print('-')\n",
        "print('Compiling...')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "inputs: integer tensor of shape (samples, max_length)\n",
            "inputs_train shape: (10000, 68)\n",
            "inputs_test shape: (1000, 68)\n",
            "-\n",
            "queries: integer tensor of shape (samples, max_length)\n",
            "queries_train shape: (10000, 4)\n",
            "queries_test shape: (1000, 4)\n",
            "-\n",
            "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
            "answers_train shape: (10000, 22)\n",
            "answers_test shape: (1000, 22)\n",
            "-\n",
            "Compiling...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvyawLqgfz_N",
        "colab_type": "text"
      },
      "source": [
        "In this part you should implement 2 functions which illustrate the procedure of learning, Loss and Accuracy. These functions take two inputs: \n",
        "* The history of your designed model \n",
        "* Proper title for describing the plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Ya5e3e3OWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install plot_keras_history\n",
        "from plot_keras_history import plot_history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_acc(history, title):\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title(title)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA9BBcEP3Hym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(history, title):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(title)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMxMeM78xBT8",
        "colab_type": "text"
      },
      "source": [
        "Define model's hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6-mw90LrKj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_epochs = 10\n",
        "batch_size = 32\n",
        "lstm_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5X8ksgrKkA",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Implementstion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayB5NMoI-Sh-",
        "colab_type": "text"
      },
      "source": [
        "Let's build the model. You should use Keras framework. The summary and outview of the right model is saved in the next cells to help you create the proper model faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UNREQ9rrKkB",
        "colab_type": "code",
        "outputId": "654e38f3-8947-43bf-f3f6-5d779ae243b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# define the model: \n",
        "\n",
        "input_sequence = Input((story_maxlen,))\n",
        "question = Input((query_maxlen,))\n",
        "\n",
        "\n",
        "sequential_4 = Sequential()\n",
        "sequential_4.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
        "sequential_4 = sequential_4(input_sequence)\n",
        "print(sequential_4.shape)\n",
        "\n",
        "sequential_5 = Sequential()\n",
        "sequential_5.add(Embedding(input_dim=vocab_size, output_dim=4))\n",
        "sequential_5 = sequential_5(input_sequence)\n",
        "print(sequential_4.shape)\n",
        "\n",
        "sequential_6 = Sequential()\n",
        "sequential_6.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=4))\n",
        "sequential_6 = sequential_6(question)\n",
        "print(sequential_6.shape)\n",
        "\n",
        "\n",
        "# dot ((?, 68, 4), (?, 4, 64))\n",
        "dot_2 = dot([sequential_4, sequential_6], axes=-1)\n",
        "dot_2 = Activation('softmax')(dot_2)\n",
        "print(dot_2.shape)\n",
        "\n",
        "\n",
        "# add((?, 4,64), (?, 68,64))\n",
        "add_2 = add([dot_2, sequential_5])\n",
        "permute_2 = Permute((2, 1))(add_2) \n",
        "\n",
        "concatenate_2 = concatenate([permute_2, sequential_6])\n",
        "\n",
        "lstm_2 = LSTM(32)(concatenate_2) \n",
        "dropout_8 = Dropout(0.5)(lstm_2)\n",
        "dense_2 = Dense(vocab_size)(dropout_8)\n",
        "\n",
        "activation_4 = Activation('softmax')(dense_2)\n",
        "answer = activation_4\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 68, 64)\n",
            "(?, 68, 64)\n",
            "(?, 4, 64)\n",
            "(?, 68, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUvfDiLkrKkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFqPorzN_9Lv",
        "colab_type": "text"
      },
      "source": [
        "This is the correct model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX6c1qWQ0iMX",
        "colab_type": "code",
        "outputId": "9fe5dec6-a712-43b2-f3b2-fff5edc9308c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model,show_shapes= True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"737pt\" viewBox=\"0.00 0.00 816.50 885.00\" width=\"680pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 881)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-881 812.5,-881 812.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139656086166608 -->\n<g class=\"node\" id=\"node1\">\n<title>139656086166608</title>\n<polygon fill=\"none\" points=\"118.5,-830.5 118.5,-876.5 389.5,-876.5 389.5,-830.5 118.5,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-849.8\">input_3: InputLayer</text>\n<polyline fill=\"none\" points=\"251.5,-830.5 251.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.5\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"251.5,-853.5 309.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.5\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"309.5,-830.5 309.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349.5\" y=\"-861.3\">(None, 68)</text>\n<polyline fill=\"none\" points=\"309.5,-853.5 389.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349.5\" y=\"-838.3\">(None, 68)</text>\n</g>\n<!-- 139656086167168 -->\n<g class=\"node\" id=\"node3\">\n<title>139656086167168</title>\n<polygon fill=\"none\" points=\"198,-747.5 198,-793.5 476,-793.5 476,-747.5 198,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276\" y=\"-766.8\">sequential_4: Sequential</text>\n<polyline fill=\"none\" points=\"354,-747.5 354,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"354,-770.5 412,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"412,-747.5 412,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-778.3\">multiple</text>\n<polyline fill=\"none\" points=\"412,-770.5 476,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-755.3\">multiple</text>\n</g>\n<!-- 139656086166608&#45;&gt;139656086167168 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139656086166608-&gt;139656086167168</title>\n<path d=\"M277.1201,-830.3799C286.2173,-821.2827 296.7912,-810.7088 306.4995,-801.0005\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"309.1198,-803.33 313.716,-793.784 304.17,-798.3802 309.1198,-803.33\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086248080 -->\n<g class=\"node\" id=\"node7\">\n<title>139656086248080</title>\n<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 278,-710.5 278,-664.5 0,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"78\" y=\"-683.8\">sequential_5: Sequential</text>\n<polyline fill=\"none\" points=\"156,-664.5 156,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"156,-687.5 214,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"214,-664.5 214,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-695.3\">multiple</text>\n<polyline fill=\"none\" points=\"214,-687.5 278,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-672.3\">multiple</text>\n</g>\n<!-- 139656086166608&#45;&gt;139656086248080 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139656086166608-&gt;139656086248080</title>\n<path d=\"M224.0913,-830.4619C212.132,-820.2132 198.8814,-807.4326 189,-794 172.3111,-771.3135 159.0949,-742.17 150.4519,-720.1062\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"153.6312,-718.6197 146.8066,-710.5157 147.0879,-721.1068 153.6312,-718.6197\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086166552 -->\n<g class=\"node\" id=\"node2\">\n<title>139656086166552</title>\n<polygon fill=\"none\" points=\"522.5,-830.5 522.5,-876.5 785.5,-876.5 785.5,-830.5 522.5,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589\" y=\"-849.8\">input_4: InputLayer</text>\n<polyline fill=\"none\" points=\"655.5,-830.5 655.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"655.5,-853.5 713.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"713.5,-830.5 713.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749.5\" y=\"-861.3\">(None, 4)</text>\n<polyline fill=\"none\" points=\"713.5,-853.5 785.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749.5\" y=\"-838.3\">(None, 4)</text>\n</g>\n<!-- 139656086240896 -->\n<g class=\"node\" id=\"node4\">\n<title>139656086240896</title>\n<polygon fill=\"none\" points=\"499.5,-747.5 499.5,-793.5 808.5,-793.5 808.5,-747.5 499.5,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"577.5\" y=\"-766.8\">sequential_6: Sequential</text>\n<polyline fill=\"none\" points=\"655.5,-747.5 655.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"655.5,-770.5 713.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"713.5,-747.5 713.5,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-778.3\">(None, 4)</text>\n<polyline fill=\"none\" points=\"713.5,-770.5 808.5,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"761\" y=\"-755.3\">(None, 4, 64)</text>\n</g>\n<!-- 139656086166552&#45;&gt;139656086240896 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139656086166552-&gt;139656086240896</title>\n<path d=\"M654,-830.3799C654,-822.1745 654,-812.7679 654,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"657.5001,-803.784 654,-793.784 650.5001,-803.784 657.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086272712 -->\n<g class=\"node\" id=\"node5\">\n<title>139656086272712</title>\n<polygon fill=\"none\" points=\"296.5,-664.5 296.5,-710.5 631.5,-710.5 631.5,-664.5 296.5,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"336.5\" y=\"-683.8\">dot_2: Dot</text>\n<polyline fill=\"none\" points=\"376.5,-664.5 376.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405.5\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"376.5,-687.5 434.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405.5\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"434.5,-664.5 434.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-695.3\">[(None, 68, 64), (None, 4, 64)]</text>\n<polyline fill=\"none\" points=\"434.5,-687.5 631.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-672.3\">(None, 68, 4)</text>\n</g>\n<!-- 139656086167168&#45;&gt;139656086272712 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139656086167168-&gt;139656086272712</title>\n<path d=\"M372.3766,-747.3799C387.1152,-737.7475 404.387,-726.4597 419.9375,-716.2967\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"421.9166,-719.1846 428.3727,-710.784 418.087,-713.325 421.9166,-719.1846\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086240896&#45;&gt;139656086272712 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139656086240896-&gt;139656086272712</title>\n<path d=\"M601.3267,-747.4901C578.0579,-737.3253 550.5306,-725.3002 526.254,-714.6952\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"527.3406,-711.3505 516.7757,-710.5547 524.5384,-717.7652 527.3406,-711.3505\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086078968 -->\n<g class=\"node\" id=\"node10\">\n<title>139656086078968</title>\n<polygon fill=\"none\" points=\"351,-332.5 351,-378.5 773,-378.5 773,-332.5 351,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-351.8\">concatenate_2: Concatenate</text>\n<polyline fill=\"none\" points=\"526,-332.5 526,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"555\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"526,-355.5 584,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"555\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"584,-332.5 584,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678.5\" y=\"-363.3\">[(None, 4, 68), (None, 4, 64)]</text>\n<polyline fill=\"none\" points=\"584,-355.5 773,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678.5\" y=\"-340.3\">(None, 4, 132)</text>\n</g>\n<!-- 139656086240896&#45;&gt;139656086078968 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139656086240896-&gt;139656086078968</title>\n<path d=\"M655.3615,-747.4147C657.1317,-715.3586 660,-655.5437 660,-604.5 660,-604.5 660,-604.5 660,-521.5 660,-472.2022 655.3505,-456.6644 629,-415 622.1107,-404.1068 612.6629,-394.0324 603.0344,-385.3878\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"605.1891,-382.6248 595.3191,-378.7731 600.6329,-387.9391 605.1891,-382.6248\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086011240 -->\n<g class=\"node\" id=\"node6\">\n<title>139656086011240</title>\n<polygon fill=\"none\" points=\"310.5,-581.5 310.5,-627.5 617.5,-627.5 617.5,-581.5 310.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"387.5\" y=\"-600.8\">activation_3: Activation</text>\n<polyline fill=\"none\" points=\"464.5,-581.5 464.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"464.5,-604.5 522.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"522.5,-581.5 522.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-612.3\">(None, 68, 4)</text>\n<polyline fill=\"none\" points=\"522.5,-604.5 617.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-589.3\">(None, 68, 4)</text>\n</g>\n<!-- 139656086272712&#45;&gt;139656086011240 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139656086272712-&gt;139656086011240</title>\n<path d=\"M464,-664.3799C464,-656.1745 464,-646.7679 464,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"467.5001,-637.784 464,-627.784 460.5001,-637.784 467.5001,-637.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086002040 -->\n<g class=\"node\" id=\"node8\">\n<title>139656086002040</title>\n<polygon fill=\"none\" points=\"298,-498.5 298,-544.5 630,-544.5 630,-498.5 298,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-517.8\">add_2: Add</text>\n<polyline fill=\"none\" points=\"383,-498.5 383,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"383,-521.5 441,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"441,-498.5 441,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"535.5\" y=\"-529.3\">[(None, 68, 4), (None, 68, 4)]</text>\n<polyline fill=\"none\" points=\"441,-521.5 630,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"535.5\" y=\"-506.3\">(None, 68, 4)</text>\n</g>\n<!-- 139656086011240&#45;&gt;139656086002040 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139656086011240-&gt;139656086002040</title>\n<path d=\"M464,-581.3799C464,-573.1745 464,-563.7679 464,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"467.5001,-554.784 464,-544.784 460.5001,-554.784 467.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086248080&#45;&gt;139656086002040 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139656086248080-&gt;139656086002040</title>\n<path d=\"M169.5876,-664.4763C201.4327,-641.2494 253.3639,-605.4226 302,-581 326.811,-568.5412 354.8853,-557.3061 380.61,-548.0412\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"382.0883,-551.2306 390.3425,-544.5884 379.7477,-544.6334 382.0883,-551.2306\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086765408 -->\n<g class=\"node\" id=\"node9\">\n<title>139656086765408</title>\n<polygon fill=\"none\" points=\"334,-415.5 334,-461.5 620,-461.5 620,-415.5 334,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400.5\" y=\"-434.8\">permute_2: Permute</text>\n<polyline fill=\"none\" points=\"467,-415.5 467,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"496\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"467,-438.5 525,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"496\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"525,-415.5 525,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"572.5\" y=\"-446.3\">(None, 68, 4)</text>\n<polyline fill=\"none\" points=\"525,-438.5 620,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"572.5\" y=\"-423.3\">(None, 4, 68)</text>\n</g>\n<!-- 139656086002040&#45;&gt;139656086765408 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139656086002040-&gt;139656086765408</title>\n<path d=\"M467.6212,-498.3799C468.9064,-490.1745 470.3797,-480.7679 471.772,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"475.2635,-472.2052 473.3531,-461.784 468.3478,-471.1219 475.2635,-472.2052\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086765408&#45;&gt;139656086078968 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139656086765408-&gt;139656086078968</title>\n<path d=\"M500.6772,-415.3799C509.9937,-406.2827 520.8223,-395.7088 530.7646,-386.0005\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"533.4454,-388.2746 538.1549,-378.784 528.5549,-383.2663 533.4454,-388.2746\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086080032 -->\n<g class=\"node\" id=\"node11\">\n<title>139656086080032</title>\n<polygon fill=\"none\" points=\"431,-249.5 431,-295.5 693,-295.5 693,-249.5 431,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482\" y=\"-268.8\">lstm_2: LSTM</text>\n<polyline fill=\"none\" points=\"533,-249.5 533,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"533,-272.5 591,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"591,-249.5 591,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"642\" y=\"-280.3\">(None, 4, 132)</text>\n<polyline fill=\"none\" points=\"591,-272.5 693,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"642\" y=\"-257.3\">(None, 32)</text>\n</g>\n<!-- 139656086078968&#45;&gt;139656086080032 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139656086078968-&gt;139656086080032</title>\n<path d=\"M562,-332.3799C562,-324.1745 562,-314.7679 562,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"565.5001,-305.784 562,-295.784 558.5001,-305.784 565.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086079584 -->\n<g class=\"node\" id=\"node12\">\n<title>139656086079584</title>\n<polygon fill=\"none\" points=\"426,-166.5 426,-212.5 698,-212.5 698,-166.5 426,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493\" y=\"-185.8\">dropout_8: Dropout</text>\n<polyline fill=\"none\" points=\"560,-166.5 560,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"560,-189.5 618,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"618,-166.5 618,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-197.3\">(None, 32)</text>\n<polyline fill=\"none\" points=\"618,-189.5 698,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-174.3\">(None, 32)</text>\n</g>\n<!-- 139656086080032&#45;&gt;139656086079584 -->\n<g class=\"edge\" id=\"edge13\">\n<title>139656086080032-&gt;139656086079584</title>\n<path d=\"M562,-249.3799C562,-241.1745 562,-231.7679 562,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"565.5001,-222.784 562,-212.784 558.5001,-222.784 565.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086080424 -->\n<g class=\"node\" id=\"node13\">\n<title>139656086080424</title>\n<polygon fill=\"none\" points=\"439.5,-83.5 439.5,-129.5 684.5,-129.5 684.5,-83.5 439.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493\" y=\"-102.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"546.5,-83.5 546.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"546.5,-106.5 604.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"604.5,-83.5 604.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"644.5\" y=\"-114.3\">(None, 32)</text>\n<polyline fill=\"none\" points=\"604.5,-106.5 684.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"644.5\" y=\"-91.3\">(None, 22)</text>\n</g>\n<!-- 139656086079584&#45;&gt;139656086080424 -->\n<g class=\"edge\" id=\"edge14\">\n<title>139656086079584-&gt;139656086080424</title>\n<path d=\"M562,-166.3799C562,-158.1745 562,-148.7679 562,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"565.5001,-139.784 562,-129.784 558.5001,-139.784 565.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139656086337856 -->\n<g class=\"node\" id=\"node14\">\n<title>139656086337856</title>\n<polygon fill=\"none\" points=\"416,-.5 416,-46.5 708,-46.5 708,-.5 416,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493\" y=\"-19.8\">activation_4: Activation</text>\n<polyline fill=\"none\" points=\"570,-.5 570,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"599\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"570,-23.5 628,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"599\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"628,-.5 628,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-31.3\">(None, 22)</text>\n<polyline fill=\"none\" points=\"628,-23.5 708,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-8.3\">(None, 22)</text>\n</g>\n<!-- 139656086080424&#45;&gt;139656086337856 -->\n<g class=\"edge\" id=\"edge15\">\n<title>139656086080424-&gt;139656086337856</title>\n<path d=\"M562,-83.3799C562,-75.1745 562,-65.7679 562,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"565.5001,-56.784 562,-46.784 558.5001,-56.784 565.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxK-MxYMrKkI",
        "colab_type": "text"
      },
      "source": [
        "Model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dloj1lP5buYK",
        "colab_type": "code",
        "outputId": "a3bb9342-ae49-4472-c022-f1c68f53e4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_99 (InputLayer)           (None, 68)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_100 (InputLayer)          (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_140 (Sequential)     multiple             1408        input_99[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_142 (Sequential)     (None, 4, 64)        1408        input_100[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dot_41 (Dot)                    (None, 68, 4)        0           sequential_140[1][0]             \n",
            "                                                                 sequential_142[1][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 68, 4)        0           dot_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "sequential_141 (Sequential)     multiple             88          input_99[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 68, 4)        0           activation_21[0][0]              \n",
            "                                                                 sequential_141[1][0]             \n",
            "__________________________________________________________________________________________________\n",
            "permute_4 (Permute)             (None, 4, 68)        0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 4, 132)       0           permute_4[0][0]                  \n",
            "                                                                 sequential_142[1][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 32)           21120       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 22)           726         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 22)           0           dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,750\n",
            "Trainable params: 24,750\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjEgTL9LrKkJ",
        "colab_type": "code",
        "outputId": "77692c62-52b0-4ea2-9673-5daf17707627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 68)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       multiple             1408        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_6 (Sequential)       (None, 4, 64)        1408        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 68, 4)        0           sequential_4[1][0]               \n",
            "                                                                 sequential_6[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 68, 4)        0           dot_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       multiple             88          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 68, 4)        0           activation_3[0][0]               \n",
            "                                                                 sequential_5[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 4, 68)        0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 132)       0           permute_2[0][0]                  \n",
            "                                                                 sequential_6[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 32)           21120       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32)           0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 22)           726         dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 22)           0           dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,750\n",
            "Trainable params: 24,750\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tnbz5YJrKkQ",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_bUUHq-A8Us",
        "colab_type": "text"
      },
      "source": [
        "In this section we start the training procedure with fitting the data to the designed model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2ve0MLeOrKkS",
        "colab_type": "code",
        "outputId": "921428b4-be08-49ea-b292-32a66055e633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "history = model.fit([inputs_train, queries_train], answers_train, batch_size, train_epochs,\n",
        "          validation_data=([inputs_test, queries_test], answers_test))\n",
        "\n",
        "plot_loss(history,\"Loss\")\n",
        "plot_acc(history,\"Accuracy\")\n",
        "\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.3196 - acc: 0.4938 - val_loss: 1.3174 - val_acc: 0.4990\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 4s 426us/step - loss: 1.2968 - acc: 0.5034 - val_loss: 1.2905 - val_acc: 0.5030\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 4s 422us/step - loss: 1.2733 - acc: 0.5092 - val_loss: 1.2433 - val_acc: 0.5040\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 4s 431us/step - loss: 1.2522 - acc: 0.5097 - val_loss: 1.2634 - val_acc: 0.5010\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.2394 - acc: 0.5150 - val_loss: 1.2054 - val_acc: 0.5190\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 4s 431us/step - loss: 1.2224 - acc: 0.5186 - val_loss: 1.2107 - val_acc: 0.5120\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 4s 428us/step - loss: 1.2098 - acc: 0.5228 - val_loss: 1.1994 - val_acc: 0.5090\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.1969 - acc: 0.5277 - val_loss: 1.1954 - val_acc: 0.5170\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.1964 - acc: 0.5238 - val_loss: 1.1814 - val_acc: 0.5110\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 4s 422us/step - loss: 1.1849 - acc: 0.5311 - val_loss: 1.1891 - val_acc: 0.5110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkoVSgNCKaVUof4/5IPReeTXMfEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4YiIiFb+Ar4wEnN1BWqZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXxhXOvcrKkV",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 Testing and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciBt077dEBLd",
        "colab_type": "text"
      },
      "source": [
        "Now we need to actually make predictions and check the performance of our trained model with some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4b3f5-arKkW",
        "colab_type": "code",
        "outputId": "a5ef201f-0a8b-4d02-d67d-ed5ba3a6647e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "for i in range(0,10):\n",
        "        current_inp = test_stories[2*i]\n",
        "        current_story, current_query, current_answer = vectorize_stories([current_inp], word_idx, story_maxlen, query_maxlen)\n",
        "        current_prediction = model.predict([current_story, current_query])\n",
        "        current_prediction = idx_word[np.argmax(current_prediction)]\n",
        "        print(' '.join(current_inp[0]), ' '.join(current_inp[1]), '| Prediction:', current_prediction, '| Ground Truth:', current_inp[2])\n",
        "        print(\"-----------------------------------------------------------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "John travelled to the hallway . Mary journeyed to the bathroom . Where is John ? | Prediction: bathroom | Ground Truth: hallway\n",
            "-----------------------------------------------------------------------------------------\n",
            "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
            "-----------------------------------------------------------------------------------------\n",
            "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Sandra travelled to the hallway . John went to the garden . Sandra went back to the bathroom . Sandra moved to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
            "-----------------------------------------------------------------------------------------\n",
            "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Where is Sandra ? | Prediction: garden | Ground Truth: garden\n",
            "-----------------------------------------------------------------------------------------\n",
            "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Daniel journeyed to the office . John moved to the hallway . Where is Sandra ? | Prediction: hallway | Ground Truth: office\n",
            "-----------------------------------------------------------------------------------------\n",
            "John travelled to the office . Mary journeyed to the kitchen . Where is Mary ? | Prediction: kitchen | Ground Truth: kitchen\n",
            "-----------------------------------------------------------------------------------------\n",
            "John travelled to the office . Mary journeyed to the kitchen . Mary moved to the garden . Daniel went to the office . John went to the hallway . Mary moved to the kitchen . Where is Daniel ? | Prediction: kitchen | Ground Truth: office\n",
            "-----------------------------------------------------------------------------------------\n",
            "John travelled to the office . Mary journeyed to the kitchen . Mary moved to the garden . Daniel went to the office . John went to the hallway . Mary moved to the kitchen . Mary travelled to the bedroom . Daniel journeyed to the garden . John journeyed to the garden . Sandra journeyed to the bedroom . Where is Mary ? | Prediction: bedroom | Ground Truth: bedroom\n",
            "-----------------------------------------------------------------------------------------\n",
            "John moved to the hallway . John journeyed to the kitchen . Sandra travelled to the garden . John journeyed to the garden . Where is John ? | Prediction: garden | Ground Truth: garden\n",
            "-----------------------------------------------------------------------------------------\n",
            "John moved to the hallway . John journeyed to the kitchen . Sandra travelled to the garden . John journeyed to the garden . Daniel journeyed to the office . John went to the kitchen . Sandra journeyed to the hallway . Mary went to the hallway . Where is Daniel ? | Prediction: hallway | Ground Truth: office\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "DWGbl4PdrKka",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 Custom Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUI2SIXEV4x",
        "colab_type": "text"
      },
      "source": [
        "You can even write your example and test it with your model to see how powerful it is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT83MJ8yrKkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('-------------------------------------------------------------------------------------------')\n",
        "print('Custom User Queries (Make sure there are spaces before each word)')\n",
        "while 1:\n",
        "    print('-------------------------------------------------------------------------------------------')\n",
        "    print('Please input a story')\n",
        "    user_story_inp = input().split(' ')\n",
        "    print('Please input a query')\n",
        "    user_query_inp = input().split(' ')\n",
        "    user_story, user_query, user_ans = vectorize_stories([[user_story_inp, user_query_inp, '.']], word_idx, story_maxlen, query_maxlen)\n",
        "    user_prediction = model.predict([user_story, user_query])\n",
        "    user_prediction = idx_word[np.argmax(user_prediction)]\n",
        "    print('Result')\n",
        "    print(' '.join(user_story_inp), ' '.join(user_query_inp), '| Prediction:', user_prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id1unEEQrKkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some examples:\n",
        "# Mary went to the bathroom . John moved to the hallway . Mary travelled to the office . # Where is Mary ?\n",
        "# Sandra travelled to the office . John journeyed to the garden ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtYRcM2KE4te",
        "colab_type": "text"
      },
      "source": [
        "As you understood how the model trained, please tell us about the pros and cons of the proposed model. How can we improve it if we want to use it in realistic task ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9NiIbN5F7zb",
        "colab_type": "text"
      },
      "source": [
        "low accuray is a cons and fast training is cons of this model.\n",
        "we can inlarge dataset and vocabulary and using NER and better vectorization methods!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9mLD2BlEcB-",
        "colab_type": "text"
      },
      "source": [
        "# 2. Sentence Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFKaoAZpGPdQ",
        "colab_type": "text"
      },
      "source": [
        "We want to find out that we can classify a sentence according to the mean of its word embeddings or not. In this section, we will find the answer to the above statement.\n",
        "\n",
        "Assume sentence $X = [\\mathbf{x^{(1)}}, ..., \\mathbf{x^{(N)}}]$ is given, then a sentence representation $\\mathbf{R}$ can be calculated as following:\n",
        "\n",
        "$$\n",
        "\\mathbf{R} = \\frac{1}{N} \\sum_{i=1}^{N} e_{x^{(i)}} \\ \\ \\mathbf{R} \\in \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "where $e_{x^{(i)}}$ is an embedding vector for the token $x^{(i)}$.\n",
        "\n",
        "\n",
        "Having such a simple model will enable us to analyze and understand its capabilities more easily. In addition, we will try one of the state-of-the-art text processing tools, called Flair, which can be run on GPUs. The task is text classification on the AG News corpus, which consists of news articles from more than 2000 news sources. Our split has 110K samples for the training and 10k for the validation set. Dataset examples are labeled with 4 major labels: `{World, Sports, Business, Sci/Tech}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBwDATv9GZ6-",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve2xVVQgGjG_",
        "colab_type": "text"
      },
      "source": [
        "Often, datasets in NLP come with unprocessed sentences. As a deep learning expert, you should be familiar with popular text processing tools such as NLTK, Spacy, Stanford CoreNLP, and Flair. Generally, text pre-processing in deep learning includes Tokenization, Vocabulary creation, and Padding. But here we want to do one more step, NER replacement. Basically, we want to replace named entities with their corresponding tags. For example \"George Washington went to New York\" will be converted to \"\\<PERSON> went to \\<LOC>\"\n",
        "\n",
        "The purpose of this step is to reduce the size of vocabulary and support more words. This strategy is proved to be most beneficial when our dataset contains a large number of named entities, e.g. News dataset. \n",
        "\n",
        "Most pre-processing parts are implemented for you. You only need to fill the following function. Be sure to read the Flair documentations first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d9fuhJ5Iy7p",
        "colab_type": "code",
        "outputId": "9b52a425-db28-497a-c015-f29d5e170df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "! wget -q https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/data.tar.gz\n",
        "! tar xvfz data.tar.gz > /dev/null\n",
        "\n",
        "from ag_news_util import read_ag_news, AG_NEWS_LBLS, create_model_input, create_vocab\n",
        "! pip install -q tqdm flair\n",
        "from tqdm import tqdm\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from IPython.display import SVG\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFjsopSEdk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tagged_string(sentence):\n",
        "  \"\"\"\n",
        "  Join tokens and replace named enitites\n",
        "  Args:\n",
        "    sentence(flair.data.Sentence): An input sentence, containing list of tokens and their NER tag\n",
        "    \n",
        "  Returns:\n",
        "    output(str): A String of sentence tokens separated by spaces and \n",
        "        each named enitity is replaced by its Tag\n",
        "  \n",
        "  Hint: Check out flair tutorials, https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md\n",
        "      sentence.get_spans('ner'), sentence.tokens, token.idx and entity.tag might be helpful.\n",
        "  \"\"\"\n",
        "  sent = sentence.to_plain_string()\n",
        "\n",
        "  for entity in sentence.get_spans('ner'):\n",
        "    sent = sent.replace(entity.text, ' <'+entity.tag+'> ')\n",
        "  \n",
        "  return \" \".join(sent.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9htIHGpVHH5b",
        "colab_type": "text"
      },
      "source": [
        "Test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akxc_dUGHCpd",
        "colab_type": "code",
        "outputId": "b786c750-3335-492d-8b86-93b59a9a70f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "s = Sentence('tomorrow, Chad asks the IMF for a loan to pay for looking after more than 100,000 refugees from conflict-torn Darfur in western Sudan.', use_tokenizer=True)\n",
        "tagger.predict(s)\n",
        "s_ner = get_tagged_string(s)\n",
        "print(s_ner == '<DATE> , <GPE> asks the <ORG> for a loan to pay for looking after <CARDINAL> refugees from conflict-torn <GPE> in western <GPE> .')\n",
        "print(s_ner)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-31 09:17:57,812 loading file /root/.flair/models/en-ner-ontonotes-v0.4.pt\n",
            "True\n",
            "<DATE> , <GPE> asks the <ORG> for a loan to pay for looking after <CARDINAL> refugees from conflict-torn <GPE> in western <GPE> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr6fyu61HP40",
        "colab_type": "text"
      },
      "source": [
        "Define model's hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1El9d7fHQYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10*1000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuGGsP0vHWCo",
        "colab_type": "text"
      },
      "source": [
        "Process the entire corpus. It will approximately take 50 minutes. Please be patient. You may want to go for the next sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZOfRG29HWl9",
        "colab_type": "code",
        "outputId": "f0a23028-6198-4aa8-83c9-9efab8aebdf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "TAGGER_BATCH_SIZE = 512\n",
        "\n",
        "if 'tagger' not in dir() or tagger is None:\n",
        "  tagger = SequenceTagger.load('ner-ontonotes')\n",
        "\n",
        "def precoess_sents(lst):\n",
        "  output = []\n",
        "  for i in tqdm(range(0, len(lst), TAGGER_BATCH_SIZE)):\n",
        "    batch = [Sentence(x, use_tokenizer=True) for x in lst[i:i + TAGGER_BATCH_SIZE]]\n",
        "    tagger.predict(batch, mini_batch_size=TAGGER_BATCH_SIZE, verbose=False)\n",
        "    batch = [get_tagged_string(s).lower() for s in batch]\n",
        "    output += batch\n",
        "    \n",
        "  return output\n",
        "\n",
        "print(\"# Download and read dataset\")\n",
        "(train_sents, train_lbls), (valid_sents, valid_lbls) = read_ag_news()\n",
        "\n",
        "print(\"\\n# Replace named entities with their corresponding tags\")\n",
        "# We need to free the gpu memory due to some unknown bug in flair library\n",
        "del tagger; tagger = SequenceTagger.load('ner-ontonotes')\n",
        "import torch; torch.cuda.empty_cache()\n",
        "train_sents_ner = precoess_sents(train_sents)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "del tagger\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "torch.cuda.empty_cache()\n",
        "valid_sents_ner = precoess_sents(valid_sents)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "assert len(train_sents_ner) == len(train_lbls)\n",
        "assert len(valid_sents_ner) == len(valid_lbls)\n",
        "\n",
        "del tagger\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "torch.cuda.empty_cache()\n",
        "del tagger\n",
        "\n",
        "print(\"# Processed dataset sample\")\n",
        "print(\"train_sents[0] =\", train_sents[0])\n",
        "print(\"train_sents_ner[0] =\", train_sents_ner[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Download and read dataset\n",
            "\n",
            "# Replace named entities with their corresponding tags\n",
            "2020-01-31 09:18:15,201 loading file /root/.flair/models/en-ner-ontonotes-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 102/215 [7:42:13<8:27:55, 269.70s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTUWKhVQHpma",
        "colab_type": "text"
      },
      "source": [
        "Create the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEvRz41kJgz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget \"http://nlp.stanford.edu/data/glove.6B.zip\" -O glove.6B.zip && unzip glove.6B.zip\n",
        "\n",
        "word2vec = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    word2vec[word] =  np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVPhJNDEJ58c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It is a good practice to initialize out-of-vocabulary tokens\n",
        "# with the embeddings' mean\n",
        "mean_embed = np.mean(np.array(list(word2vec.values())), axis=0)\n",
        "\n",
        "# Create the embedding matrix according to our vocabulary\n",
        "embedding_matrix = np.zeros((len(tok2id), EMBEDDING_DIM))\n",
        "for word, i in tok2id.items():\n",
        "  embedding_matrix[i] = word2vec.get(word, mean_embed)\n",
        "  \n",
        "print(\"embedding_matrix.shape =\", embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp-Yww-cHqpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First create the vocabulary\n",
        "vocab = create_vocab(train_sents_ner, VOCAB_SIZE)\n",
        "tok2id = {w:i for i, w in enumerate(vocab)}\n",
        "\n",
        "# It is a good practice to initialize out-of-vocabulary tokens\n",
        "# with the embedding matrix mean\n",
        "mean_embed = np.mean(np.array(list(word2vec.values())), axis=0)\n",
        "\n",
        "# Create the embedding matrix according to the vocabulary\n",
        "embedding_matrix = np.zeros((len(tok2id), EMBEDDING_DIM))\n",
        "for word, i in tok2id.items():\n",
        "  embedding_matrix[i] = word2vec.get(word, mean_embed)\n",
        "\n",
        "# Fill index 0 with zero values: padding word vector\n",
        "embedding_matrix[0] = np.zeros(shape=(EMBEDDING_DIM, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aicP42i_KlEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the model input\n",
        "x_train, y_train = create_model_input(train_sents_ner, tok2id, MAX_LEN), to_categorical(train_lbls, NUM_CLASSES)\n",
        "x_valid, y_valid = create_model_input(valid_sents_ner, tok2id, MAX_LEN), to_categorical(valid_lbls, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c3tfyj_KsQA",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJ4UfOaKzkW",
        "colab_type": "text"
      },
      "source": [
        "Let's build the model. As always Keras functional API is recommended. Numeber of layer as well as their dimensionality is totally up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ZdDj3HKtwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glaDYddRLBJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We define one hypothetical layer as hint that can help you how to complete this class.\n",
        "\n",
        "class BowModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(BowModel, self).__init__(name='bow')\n",
        "    \n",
        "    self.model = Sequential()\n",
        "    self.model.add(Embedding(max_features, output_dim=256))\n",
        "    self.model.add(Dense(200), activation='relu')\n",
        "\n",
        "  \n",
        "  def call(self, words):\n",
        "    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "      words(Tensor): An input tensor for word ids with shape (?, MAX_LEN)\n",
        "    \"\"\"\n",
        "    self.model.add(Embedding(words, output_dim=256))\n",
        "    self.model.add(Dense(256,activation='relu'))\n",
        "\n",
        "    \n",
        "    return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_nx4xLlMR4P",
        "colab_type": "text"
      },
      "source": [
        "Creating model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXq4LgmIMRbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create and visualize the NER model\n",
        "bow_model = BowModel()\n",
        "bow_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "SVG(model_to_dot(bow_model,show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zJqJ3G8MdFG",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1TDD1VnMeZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and visualize training\n",
        "bow_model_hist = bow_model.fit(\n",
        "    x_train, y_train, \n",
        "    batch_size=BATCH_SIZE, epochs=10, \n",
        "    validation_data=(x_valid, y_valid)\n",
        ")\n",
        "plot_acc(bow_model_hist, \"Accracy of the model\")\n",
        "plot_loss(bow_model_hist, \"Loss of the model\")\n",
        "bow_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIqlPueGOND-",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG3vR_58OYfi",
        "colab_type": "text"
      },
      "source": [
        "An interactive shell is provided below. You can enter an input sequence to get the predicted label. The preprocessing functions will do the tokenization, thus don't worry about the spacing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY14I6rFOOT4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Interactive Shell\n",
        "if 'tagger' not in dir() or tagger is None:\n",
        "  tagger = SequenceTagger.load('ner-ontonotes')\n",
        "\n",
        "input_text = \"\"#@param {type:\"string\"}\n",
        "input_sents_ner = precoess_sents([input_text])\n",
        "input_tensor = create_model_input(input_sents_ner, tok2id, MAX_LEN)\n",
        "pred_label = bow_model.predict(input_tensor)\n",
        "\n",
        "print(\"\\n-----\\n\\n    x: \", input_text)\n",
        "print(\"x_ner: \", input_sents_ner[0])\n",
        "print(\"\\n   y': \", AG_NEWS_LBLS[np.argmax(pred_label[0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7vIbDJfRgtM",
        "colab_type": "text"
      },
      "source": [
        "It is always helpful to see the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JzbCITGRhPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "yp_valid = bow_model.predict(x_valid)\n",
        "yp_valid_ids = np.argmax(yp_valid, axis=1)\n",
        "y_valid_ids = np.argmax(y_valid, axis=1)\n",
        "print(\"\\n# Classification Report\")\n",
        "print(classification_report(y_valid_ids, yp_valid_ids, target_names=AG_NEWS_LBLS))\n",
        "\n",
        "print(\"# Confusion Matrix\")\n",
        "cm = confusion_matrix(y_valid_ids, yp_valid_ids)\n",
        "plot_confusion_matrix(cm, AG_NEWS_LBLS, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhFpuRJJQW_B",
        "colab_type": "text"
      },
      "source": [
        "Obviously, this is a relatively simple model. Hence it has limited modeling capabilities; Now it's time to find its mistakes. Can you fool the model by feeding a toxic example? Can you see the bag-of-word effect in its behavior? Write down the model limitation, Answers to the above questions, and keep in mind that you need to support each of your thoughts with an input/output example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb-BN-eUQesm",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5CLFbBWPHbh",
        "colab_type": "text"
      },
      "source": [
        "# 3. Generating Text with Recurrent Neural Networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPTQgEG9PgQe",
        "colab_type": "text"
      },
      "source": [
        "Recurrent neural networks can also be used as generative models. This means that in addition to being used for predictive models (making predictions) they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain. Generative models are useful not only to study how well a model has learned a problem, but to learn more about the problem domain itself. In this part, we want to make a model which can generate texts with its gathered knowledge. \n",
        "\n",
        "We are going to learn the dependencies between words and the conditional probabilities of words in sequences, so that we can in turn generate wholly new and original sequences of characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqBZkjmUTf5o",
        "colab_type": "code",
        "outputId": "ff5a93aa-cb75-4a2b-f426-c5040aba8bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "import numpy as np \n",
        "from IPython.display import SVG\n",
        "from keras.utils import model_to_dot\n",
        "\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsCsXrwhRQT1",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FLw8VtORc-d",
        "colab_type": "text"
      },
      "source": [
        "Generation models and their experiments are not limited to text, we can also experiment with other data, such as computer source code, marked up documents in LaTeX, HTML or Markdown and more. As mentioned before, we want to work with text data in this section. Our dataset is raw english and simple text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5xlm9fQXwp",
        "colab_type": "code",
        "outputId": "61d0eb2a-8545-4b8d-9619-cc4a6be89735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-TnqUNtUwJwcGM6NmrcFnRxAhSs3fqKB' -O EngSentences.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-30 19:05:51--  https://docs.google.com/uc?export=download&id=1-TnqUNtUwJwcGM6NmrcFnRxAhSs3fqKB\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.100, 74.125.195.139, 74.125.195.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-0c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/veria7trq6tmg98jnrhq9uq7shsg9pkk/1580407200000/04607754120182374769/*/1-TnqUNtUwJwcGM6NmrcFnRxAhSs3fqKB?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-01-30 19:05:51--  https://doc-10-0c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/veria7trq6tmg98jnrhq9uq7shsg9pkk/1580407200000/04607754120182374769/*/1-TnqUNtUwJwcGM6NmrcFnRxAhSs3fqKB?e=download\n",
            "Resolving doc-10-0c-docs.googleusercontent.com (doc-10-0c-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-10-0c-docs.googleusercontent.com (doc-10-0c-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25846 (25K) [text/plain]\n",
            "Saving to: ‘EngSentences.txt’\n",
            "\n",
            "\rEngSentences.txt      0%[                    ]       0  --.-KB/s               \rEngSentences.txt    100%[===================>]  25.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-30 19:05:52 (150 MB/s) - ‘EngSentences.txt’ saved [25846/25846]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJD19EMrVe91",
        "colab_type": "text"
      },
      "source": [
        "Now we need to prepare the dataset ready for modeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhuEUyv4S8gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataset_preparation(data):\n",
        "\n",
        "\t# basic cleanup\n",
        "\tcorpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\t# tokenization\t\n",
        "\ttokenizer.fit_on_texts(corpus)\n",
        "\tall_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\t# create input sequences using list of tokens\n",
        "\tinput_sequences = []\n",
        "\tfor line in corpus:\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\t\tfor i in range(1, len(token_list)):\n",
        "\t\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\t# pad sequences \n",
        "\tmax_sequence_len = max([len(x) for x in input_sequences])\n",
        "\tinput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "\t# create inputs and labels\n",
        "\tinputs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\tlabels = ku.to_categorical(labels, num_classes=all_words)\n",
        "\n",
        "\treturn inputs, labels, max_sequence_len, all_words\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZij5HwCVxXM",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Implementation and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FyneTvIWeRm",
        "colab_type": "text"
      },
      "source": [
        "Let's build the model. Using Keras framework is recommended. Number of layer as well as their dimensionality is totally up to you but the outview of the prefered model is saved for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqqCzdKHV_0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(inputs, labels, max_sequence_len, all_words):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(all_words, max_sequence_len-1,input_length=max_sequence_len-1))\n",
        "  model.add(LSTM(140,return_sequences=True))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(all_words,activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  history = model.fit(inputs,labels,batch_size=32,validation_split=0.33,nb_epoch=500,verbose=1)\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5g7AsfIr10N",
        "colab_type": "text"
      },
      "source": [
        "It's time to train the model and see the result. With big number of epochs, the training will take a long time. But the result will be better. So please be patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifj0L7y3vL-",
        "colab_type": "code",
        "outputId": "8d99cb4a-17a4-4656-881d-9ffb39f50432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "data = open('EngSentences.txt').read()\n",
        "inputs, labels, max_sequence_len, all_words = dataset_preparation(data)\n",
        "print(inputs.shape)\n",
        "print(labels.shape)\n",
        "print(max_sequence_len)\n",
        "print((all_words))\n",
        "print(len(data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4666, 21)\n",
            "(4666, 893)\n",
            "22\n",
            "893\n",
            "25846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFlySHv3qnzT",
        "colab_type": "code",
        "outputId": "f28927d8-58ad-4e7e-fc34-cb50c7e0057a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data = open('EngSentences.txt').read()\n",
        "inputs, labels, max_sequence_len, all_words = dataset_preparation(data)\n",
        "model3,history3 = create_model(inputs, labels, max_sequence_len, all_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 21, 21)            18753     \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 21, 140)           90720     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 21, 140)           0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100)               96400     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 893)               90193     \n",
            "=================================================================\n",
            "Total params: 306,166\n",
            "Trainable params: 306,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3126 samples, validate on 1540 samples\n",
            "Epoch 1/500\n",
            "3126/3126 [==============================] - 12s 4ms/step - loss: 5.5381 - acc: 0.0966 - val_loss: 5.4096 - val_acc: 0.1013\n",
            "Epoch 2/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 5.2231 - acc: 0.1046 - val_loss: 5.4953 - val_acc: 0.1013\n",
            "Epoch 3/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 5.1061 - acc: 0.1075 - val_loss: 5.4208 - val_acc: 0.1013\n",
            "Epoch 4/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.9325 - acc: 0.1222 - val_loss: 5.3419 - val_acc: 0.1636\n",
            "Epoch 5/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.6414 - acc: 0.1740 - val_loss: 5.1098 - val_acc: 0.1656\n",
            "Epoch 6/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.4575 - acc: 0.1763 - val_loss: 5.1858 - val_acc: 0.1753\n",
            "Epoch 7/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.3132 - acc: 0.1878 - val_loss: 5.0535 - val_acc: 0.1799\n",
            "Epoch 8/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.1819 - acc: 0.1926 - val_loss: 5.0580 - val_acc: 0.1916\n",
            "Epoch 9/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 4.0709 - acc: 0.2179 - val_loss: 5.0255 - val_acc: 0.2019\n",
            "Epoch 10/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.9433 - acc: 0.2252 - val_loss: 4.9988 - val_acc: 0.2078\n",
            "Epoch 11/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.8352 - acc: 0.2313 - val_loss: 4.9588 - val_acc: 0.2136\n",
            "Epoch 12/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 3.7375 - acc: 0.2454 - val_loss: 4.9026 - val_acc: 0.2234\n",
            "Epoch 13/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.6607 - acc: 0.2463 - val_loss: 4.9789 - val_acc: 0.2299\n",
            "Epoch 14/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.5735 - acc: 0.2566 - val_loss: 4.9626 - val_acc: 0.2266\n",
            "Epoch 15/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.5188 - acc: 0.2601 - val_loss: 5.0769 - val_acc: 0.2273\n",
            "Epoch 16/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.4462 - acc: 0.2636 - val_loss: 5.0827 - val_acc: 0.2305\n",
            "Epoch 17/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.3928 - acc: 0.2665 - val_loss: 5.1557 - val_acc: 0.2338\n",
            "Epoch 18/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 3.3324 - acc: 0.2684 - val_loss: 5.1018 - val_acc: 0.2318\n",
            "Epoch 19/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.2834 - acc: 0.2719 - val_loss: 5.1283 - val_acc: 0.2338\n",
            "Epoch 20/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.2329 - acc: 0.2767 - val_loss: 5.2298 - val_acc: 0.2370\n",
            "Epoch 21/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.1883 - acc: 0.2841 - val_loss: 5.1488 - val_acc: 0.2312\n",
            "Epoch 22/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.1333 - acc: 0.2879 - val_loss: 5.2986 - val_acc: 0.2312\n",
            "Epoch 23/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.0865 - acc: 0.2911 - val_loss: 5.3946 - val_acc: 0.2370\n",
            "Epoch 24/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 3.0381 - acc: 0.2959 - val_loss: 5.3891 - val_acc: 0.2455\n",
            "Epoch 25/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.9953 - acc: 0.2988 - val_loss: 5.3523 - val_acc: 0.2416\n",
            "Epoch 26/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.9592 - acc: 0.2994 - val_loss: 5.4830 - val_acc: 0.2396\n",
            "Epoch 27/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.9214 - acc: 0.3052 - val_loss: 5.4659 - val_acc: 0.2383\n",
            "Epoch 28/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.8766 - acc: 0.3113 - val_loss: 5.5072 - val_acc: 0.2357\n",
            "Epoch 29/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.8345 - acc: 0.3122 - val_loss: 5.5326 - val_acc: 0.2279\n",
            "Epoch 30/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.7967 - acc: 0.3237 - val_loss: 5.5989 - val_acc: 0.2318\n",
            "Epoch 31/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.7505 - acc: 0.3196 - val_loss: 5.6484 - val_acc: 0.2364\n",
            "Epoch 32/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.7173 - acc: 0.3301 - val_loss: 5.6581 - val_acc: 0.2247\n",
            "Epoch 33/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.6881 - acc: 0.3289 - val_loss: 5.7434 - val_acc: 0.2221\n",
            "Epoch 34/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.6402 - acc: 0.3337 - val_loss: 5.7350 - val_acc: 0.2357\n",
            "Epoch 35/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.6016 - acc: 0.3480 - val_loss: 5.8158 - val_acc: 0.2312\n",
            "Epoch 36/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.5688 - acc: 0.3493 - val_loss: 5.7989 - val_acc: 0.2240\n",
            "Epoch 37/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.5226 - acc: 0.3480 - val_loss: 5.8872 - val_acc: 0.2383\n",
            "Epoch 38/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.4869 - acc: 0.3557 - val_loss: 5.8594 - val_acc: 0.2279\n",
            "Epoch 39/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.4625 - acc: 0.3599 - val_loss: 5.9916 - val_acc: 0.2331\n",
            "Epoch 40/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.4272 - acc: 0.3660 - val_loss: 5.9613 - val_acc: 0.2253\n",
            "Epoch 41/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.3866 - acc: 0.3688 - val_loss: 6.0958 - val_acc: 0.2305\n",
            "Epoch 42/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.3507 - acc: 0.3772 - val_loss: 6.0862 - val_acc: 0.2312\n",
            "Epoch 43/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.3169 - acc: 0.3816 - val_loss: 6.0947 - val_acc: 0.2214\n",
            "Epoch 44/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.2845 - acc: 0.3884 - val_loss: 6.1727 - val_acc: 0.2292\n",
            "Epoch 45/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.2544 - acc: 0.3954 - val_loss: 6.1521 - val_acc: 0.2292\n",
            "Epoch 46/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.2073 - acc: 0.4095 - val_loss: 6.1937 - val_acc: 0.2357\n",
            "Epoch 47/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.1904 - acc: 0.4095 - val_loss: 6.2992 - val_acc: 0.2364\n",
            "Epoch 48/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.1530 - acc: 0.4136 - val_loss: 6.3899 - val_acc: 0.2305\n",
            "Epoch 49/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.1142 - acc: 0.4245 - val_loss: 6.4075 - val_acc: 0.2286\n",
            "Epoch 50/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.0916 - acc: 0.4344 - val_loss: 6.4422 - val_acc: 0.2286\n",
            "Epoch 51/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 2.0589 - acc: 0.4440 - val_loss: 6.4920 - val_acc: 0.2312\n",
            "Epoch 52/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.0324 - acc: 0.4459 - val_loss: 6.5407 - val_acc: 0.2305\n",
            "Epoch 53/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 2.0164 - acc: 0.4527 - val_loss: 6.5463 - val_acc: 0.2266\n",
            "Epoch 54/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.9830 - acc: 0.4639 - val_loss: 6.5736 - val_acc: 0.2195\n",
            "Epoch 55/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.9539 - acc: 0.4664 - val_loss: 6.6055 - val_acc: 0.2182\n",
            "Epoch 56/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.9305 - acc: 0.4766 - val_loss: 6.6696 - val_acc: 0.2299\n",
            "Epoch 57/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.8993 - acc: 0.4824 - val_loss: 6.7305 - val_acc: 0.2234\n",
            "Epoch 58/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.8810 - acc: 0.4843 - val_loss: 6.7675 - val_acc: 0.2299\n",
            "Epoch 59/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.8519 - acc: 0.4942 - val_loss: 6.7406 - val_acc: 0.2273\n",
            "Epoch 60/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.8359 - acc: 0.5045 - val_loss: 6.8505 - val_acc: 0.2201\n",
            "Epoch 61/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.8057 - acc: 0.5070 - val_loss: 6.9280 - val_acc: 0.2325\n",
            "Epoch 62/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.7885 - acc: 0.5112 - val_loss: 6.9105 - val_acc: 0.2338\n",
            "Epoch 63/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.7769 - acc: 0.5195 - val_loss: 6.9562 - val_acc: 0.2292\n",
            "Epoch 64/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.7627 - acc: 0.5182 - val_loss: 6.9303 - val_acc: 0.2318\n",
            "Epoch 65/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.7306 - acc: 0.5333 - val_loss: 6.9584 - val_acc: 0.2318\n",
            "Epoch 66/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.7086 - acc: 0.5406 - val_loss: 7.0689 - val_acc: 0.2214\n",
            "Epoch 67/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.6880 - acc: 0.5429 - val_loss: 7.1388 - val_acc: 0.2234\n",
            "Epoch 68/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.6608 - acc: 0.5470 - val_loss: 7.1099 - val_acc: 0.2305\n",
            "Epoch 69/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.6570 - acc: 0.5582 - val_loss: 7.1161 - val_acc: 0.2221\n",
            "Epoch 70/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.6361 - acc: 0.5637 - val_loss: 7.1813 - val_acc: 0.2286\n",
            "Epoch 71/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.6103 - acc: 0.5697 - val_loss: 7.1897 - val_acc: 0.2273\n",
            "Epoch 72/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5976 - acc: 0.5729 - val_loss: 7.2314 - val_acc: 0.2227\n",
            "Epoch 73/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5832 - acc: 0.5659 - val_loss: 7.2455 - val_acc: 0.2260\n",
            "Epoch 74/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5691 - acc: 0.5765 - val_loss: 7.2751 - val_acc: 0.2221\n",
            "Epoch 75/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5625 - acc: 0.5777 - val_loss: 7.3539 - val_acc: 0.2299\n",
            "Epoch 76/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.5432 - acc: 0.5835 - val_loss: 7.3095 - val_acc: 0.2247\n",
            "Epoch 77/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5107 - acc: 0.5956 - val_loss: 7.3255 - val_acc: 0.2331\n",
            "Epoch 78/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.5026 - acc: 0.5976 - val_loss: 7.3409 - val_acc: 0.2214\n",
            "Epoch 79/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.5028 - acc: 0.5928 - val_loss: 7.4464 - val_acc: 0.2201\n",
            "Epoch 80/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.4841 - acc: 0.6065 - val_loss: 7.3996 - val_acc: 0.2286\n",
            "Epoch 81/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.4589 - acc: 0.6184 - val_loss: 7.4383 - val_acc: 0.2266\n",
            "Epoch 82/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.4612 - acc: 0.6132 - val_loss: 7.4504 - val_acc: 0.2208\n",
            "Epoch 83/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.4395 - acc: 0.6129 - val_loss: 7.4515 - val_acc: 0.2253\n",
            "Epoch 84/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.4316 - acc: 0.6161 - val_loss: 7.5336 - val_acc: 0.2260\n",
            "Epoch 85/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.4306 - acc: 0.6180 - val_loss: 7.5058 - val_acc: 0.2247\n",
            "Epoch 86/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.4060 - acc: 0.6289 - val_loss: 7.4996 - val_acc: 0.2292\n",
            "Epoch 87/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3979 - acc: 0.6254 - val_loss: 7.5980 - val_acc: 0.2273\n",
            "Epoch 88/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3946 - acc: 0.6276 - val_loss: 7.5483 - val_acc: 0.2266\n",
            "Epoch 89/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.3697 - acc: 0.6401 - val_loss: 7.6253 - val_acc: 0.2273\n",
            "Epoch 90/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3547 - acc: 0.6366 - val_loss: 7.6649 - val_acc: 0.2195\n",
            "Epoch 91/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3524 - acc: 0.6369 - val_loss: 7.6309 - val_acc: 0.2117\n",
            "Epoch 92/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3414 - acc: 0.6392 - val_loss: 7.6763 - val_acc: 0.2214\n",
            "Epoch 93/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3321 - acc: 0.6436 - val_loss: 7.6836 - val_acc: 0.2286\n",
            "Epoch 94/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.3218 - acc: 0.6462 - val_loss: 7.6633 - val_acc: 0.2169\n",
            "Epoch 95/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.3096 - acc: 0.6510 - val_loss: 7.7186 - val_acc: 0.2305\n",
            "Epoch 96/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.2919 - acc: 0.6539 - val_loss: 7.7020 - val_acc: 0.2214\n",
            "Epoch 97/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2992 - acc: 0.6555 - val_loss: 7.7718 - val_acc: 0.2240\n",
            "Epoch 98/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2813 - acc: 0.6593 - val_loss: 7.7742 - val_acc: 0.2240\n",
            "Epoch 99/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.2772 - acc: 0.6587 - val_loss: 7.8057 - val_acc: 0.2240\n",
            "Epoch 100/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2643 - acc: 0.6654 - val_loss: 7.7747 - val_acc: 0.2331\n",
            "Epoch 101/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2582 - acc: 0.6686 - val_loss: 7.7910 - val_acc: 0.2234\n",
            "Epoch 102/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2533 - acc: 0.6663 - val_loss: 7.8890 - val_acc: 0.2279\n",
            "Epoch 103/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2362 - acc: 0.6779 - val_loss: 7.8897 - val_acc: 0.2292\n",
            "Epoch 104/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2359 - acc: 0.6721 - val_loss: 7.8865 - val_acc: 0.2260\n",
            "Epoch 105/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2315 - acc: 0.6779 - val_loss: 7.8876 - val_acc: 0.2312\n",
            "Epoch 106/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2198 - acc: 0.6702 - val_loss: 7.9017 - val_acc: 0.2227\n",
            "Epoch 107/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2058 - acc: 0.6804 - val_loss: 7.9561 - val_acc: 0.2260\n",
            "Epoch 108/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.2016 - acc: 0.6785 - val_loss: 7.9414 - val_acc: 0.2260\n",
            "Epoch 109/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1988 - acc: 0.6775 - val_loss: 7.9415 - val_acc: 0.2227\n",
            "Epoch 110/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1859 - acc: 0.6881 - val_loss: 7.9740 - val_acc: 0.2221\n",
            "Epoch 111/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1854 - acc: 0.6897 - val_loss: 7.9955 - val_acc: 0.2240\n",
            "Epoch 112/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.1820 - acc: 0.6878 - val_loss: 8.0129 - val_acc: 0.2234\n",
            "Epoch 113/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.1724 - acc: 0.6827 - val_loss: 7.9796 - val_acc: 0.2221\n",
            "Epoch 114/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1709 - acc: 0.6862 - val_loss: 8.0371 - val_acc: 0.2266\n",
            "Epoch 115/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1695 - acc: 0.6894 - val_loss: 8.0412 - val_acc: 0.2260\n",
            "Epoch 116/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1504 - acc: 0.6907 - val_loss: 8.0562 - val_acc: 0.2273\n",
            "Epoch 117/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.1453 - acc: 0.6993 - val_loss: 8.0975 - val_acc: 0.2195\n",
            "Epoch 118/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1413 - acc: 0.6945 - val_loss: 8.1120 - val_acc: 0.2201\n",
            "Epoch 119/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1335 - acc: 0.6987 - val_loss: 8.1253 - val_acc: 0.2253\n",
            "Epoch 120/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1230 - acc: 0.7028 - val_loss: 8.1478 - val_acc: 0.2260\n",
            "Epoch 121/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.1294 - acc: 0.6961 - val_loss: 8.1302 - val_acc: 0.2247\n",
            "Epoch 122/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1124 - acc: 0.7009 - val_loss: 8.1687 - val_acc: 0.2266\n",
            "Epoch 123/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1224 - acc: 0.7057 - val_loss: 8.1482 - val_acc: 0.2299\n",
            "Epoch 124/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.1174 - acc: 0.6996 - val_loss: 8.1533 - val_acc: 0.2208\n",
            "Epoch 125/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.1040 - acc: 0.7067 - val_loss: 8.1768 - val_acc: 0.2182\n",
            "Epoch 126/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0947 - acc: 0.7124 - val_loss: 8.2097 - val_acc: 0.2286\n",
            "Epoch 127/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.0989 - acc: 0.7015 - val_loss: 8.2128 - val_acc: 0.2266\n",
            "Epoch 128/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0797 - acc: 0.7108 - val_loss: 8.1817 - val_acc: 0.2201\n",
            "Epoch 129/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.0695 - acc: 0.7127 - val_loss: 8.1887 - val_acc: 0.2260\n",
            "Epoch 130/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0765 - acc: 0.7137 - val_loss: 8.1905 - val_acc: 0.2162\n",
            "Epoch 131/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0653 - acc: 0.7095 - val_loss: 8.2124 - val_acc: 0.2234\n",
            "Epoch 132/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0508 - acc: 0.7207 - val_loss: 8.2401 - val_acc: 0.2260\n",
            "Epoch 133/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0547 - acc: 0.7175 - val_loss: 8.2637 - val_acc: 0.2266\n",
            "Epoch 134/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0553 - acc: 0.7137 - val_loss: 8.2933 - val_acc: 0.2253\n",
            "Epoch 135/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0474 - acc: 0.7210 - val_loss: 8.3431 - val_acc: 0.2247\n",
            "Epoch 136/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.0432 - acc: 0.7163 - val_loss: 8.3101 - val_acc: 0.2195\n",
            "Epoch 137/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 1.0335 - acc: 0.7121 - val_loss: 8.3497 - val_acc: 0.2169\n",
            "Epoch 138/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0419 - acc: 0.7147 - val_loss: 8.3145 - val_acc: 0.2195\n",
            "Epoch 139/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0269 - acc: 0.7271 - val_loss: 8.3451 - val_acc: 0.2175\n",
            "Epoch 140/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0338 - acc: 0.7201 - val_loss: 8.3366 - val_acc: 0.2201\n",
            "Epoch 141/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0283 - acc: 0.7249 - val_loss: 8.3512 - val_acc: 0.2149\n",
            "Epoch 142/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0302 - acc: 0.7258 - val_loss: 8.3466 - val_acc: 0.2136\n",
            "Epoch 143/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0160 - acc: 0.7313 - val_loss: 8.3320 - val_acc: 0.2273\n",
            "Epoch 144/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0132 - acc: 0.7242 - val_loss: 8.3837 - val_acc: 0.2279\n",
            "Epoch 145/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0072 - acc: 0.7348 - val_loss: 8.3592 - val_acc: 0.2227\n",
            "Epoch 146/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0024 - acc: 0.7300 - val_loss: 8.3770 - val_acc: 0.2221\n",
            "Epoch 147/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9989 - acc: 0.7290 - val_loss: 8.3923 - val_acc: 0.2299\n",
            "Epoch 148/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 1.0067 - acc: 0.7265 - val_loss: 8.4456 - val_acc: 0.2169\n",
            "Epoch 149/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9915 - acc: 0.7342 - val_loss: 8.4244 - val_acc: 0.2201\n",
            "Epoch 150/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9831 - acc: 0.7361 - val_loss: 8.4255 - val_acc: 0.2299\n",
            "Epoch 151/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9831 - acc: 0.7402 - val_loss: 8.4140 - val_acc: 0.2331\n",
            "Epoch 152/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9872 - acc: 0.7316 - val_loss: 8.4389 - val_acc: 0.2234\n",
            "Epoch 153/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9804 - acc: 0.7418 - val_loss: 8.4399 - val_acc: 0.2188\n",
            "Epoch 154/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9778 - acc: 0.7396 - val_loss: 8.4828 - val_acc: 0.2169\n",
            "Epoch 155/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9748 - acc: 0.7390 - val_loss: 8.4690 - val_acc: 0.2318\n",
            "Epoch 156/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9697 - acc: 0.7409 - val_loss: 8.4703 - val_acc: 0.2208\n",
            "Epoch 157/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9550 - acc: 0.7470 - val_loss: 8.4969 - val_acc: 0.2221\n",
            "Epoch 158/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9566 - acc: 0.7431 - val_loss: 8.4914 - val_acc: 0.2162\n",
            "Epoch 159/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9619 - acc: 0.7367 - val_loss: 8.4807 - val_acc: 0.2214\n",
            "Epoch 160/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9494 - acc: 0.7396 - val_loss: 8.4806 - val_acc: 0.2175\n",
            "Epoch 161/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9517 - acc: 0.7450 - val_loss: 8.5553 - val_acc: 0.2240\n",
            "Epoch 162/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9468 - acc: 0.7390 - val_loss: 8.5209 - val_acc: 0.2227\n",
            "Epoch 163/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9356 - acc: 0.7447 - val_loss: 8.5214 - val_acc: 0.2240\n",
            "Epoch 164/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9405 - acc: 0.7457 - val_loss: 8.5285 - val_acc: 0.2266\n",
            "Epoch 165/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9440 - acc: 0.7399 - val_loss: 8.6131 - val_acc: 0.2221\n",
            "Epoch 166/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9273 - acc: 0.7444 - val_loss: 8.5897 - val_acc: 0.2253\n",
            "Epoch 167/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9258 - acc: 0.7454 - val_loss: 8.5911 - val_acc: 0.2234\n",
            "Epoch 168/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9199 - acc: 0.7527 - val_loss: 8.5716 - val_acc: 0.2292\n",
            "Epoch 169/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9232 - acc: 0.7422 - val_loss: 8.6407 - val_acc: 0.2299\n",
            "Epoch 170/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9218 - acc: 0.7524 - val_loss: 8.6462 - val_acc: 0.2188\n",
            "Epoch 171/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9181 - acc: 0.7450 - val_loss: 8.5919 - val_acc: 0.2214\n",
            "Epoch 172/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9046 - acc: 0.7495 - val_loss: 8.6311 - val_acc: 0.2227\n",
            "Epoch 173/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9157 - acc: 0.7502 - val_loss: 8.6082 - val_acc: 0.2221\n",
            "Epoch 174/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9029 - acc: 0.7550 - val_loss: 8.6374 - val_acc: 0.2188\n",
            "Epoch 175/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8896 - acc: 0.7566 - val_loss: 8.6523 - val_acc: 0.2273\n",
            "Epoch 176/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.9059 - acc: 0.7540 - val_loss: 8.6536 - val_acc: 0.2260\n",
            "Epoch 177/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8934 - acc: 0.7594 - val_loss: 8.6494 - val_acc: 0.2253\n",
            "Epoch 178/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.9002 - acc: 0.7575 - val_loss: 8.7062 - val_acc: 0.2195\n",
            "Epoch 179/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8962 - acc: 0.7601 - val_loss: 8.7034 - val_acc: 0.2221\n",
            "Epoch 180/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8939 - acc: 0.7566 - val_loss: 8.7196 - val_acc: 0.2195\n",
            "Epoch 181/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8863 - acc: 0.7607 - val_loss: 8.7061 - val_acc: 0.2175\n",
            "Epoch 182/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8801 - acc: 0.7617 - val_loss: 8.7246 - val_acc: 0.2156\n",
            "Epoch 183/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8803 - acc: 0.7614 - val_loss: 8.7211 - val_acc: 0.2234\n",
            "Epoch 184/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8808 - acc: 0.7614 - val_loss: 8.7163 - val_acc: 0.2208\n",
            "Epoch 185/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8806 - acc: 0.7537 - val_loss: 8.6943 - val_acc: 0.2175\n",
            "Epoch 186/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8734 - acc: 0.7591 - val_loss: 8.7551 - val_acc: 0.2234\n",
            "Epoch 187/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8705 - acc: 0.7655 - val_loss: 8.8056 - val_acc: 0.2175\n",
            "Epoch 188/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8795 - acc: 0.7598 - val_loss: 8.8343 - val_acc: 0.2260\n",
            "Epoch 189/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8810 - acc: 0.7598 - val_loss: 8.7816 - val_acc: 0.2221\n",
            "Epoch 190/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8767 - acc: 0.7598 - val_loss: 8.7827 - val_acc: 0.2253\n",
            "Epoch 191/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8672 - acc: 0.7658 - val_loss: 8.7780 - val_acc: 0.2208\n",
            "Epoch 192/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8586 - acc: 0.7687 - val_loss: 8.7827 - val_acc: 0.2182\n",
            "Epoch 193/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8598 - acc: 0.7652 - val_loss: 8.8046 - val_acc: 0.2208\n",
            "Epoch 194/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8600 - acc: 0.7652 - val_loss: 8.9140 - val_acc: 0.2240\n",
            "Epoch 195/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8657 - acc: 0.7662 - val_loss: 8.8522 - val_acc: 0.2208\n",
            "Epoch 196/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8483 - acc: 0.7742 - val_loss: 8.8400 - val_acc: 0.2299\n",
            "Epoch 197/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8592 - acc: 0.7738 - val_loss: 8.8384 - val_acc: 0.2221\n",
            "Epoch 198/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8439 - acc: 0.7722 - val_loss: 8.8233 - val_acc: 0.2273\n",
            "Epoch 199/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8485 - acc: 0.7674 - val_loss: 8.7837 - val_acc: 0.2325\n",
            "Epoch 200/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8418 - acc: 0.7687 - val_loss: 8.8145 - val_acc: 0.2299\n",
            "Epoch 201/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8419 - acc: 0.7687 - val_loss: 8.8735 - val_acc: 0.2299\n",
            "Epoch 202/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8434 - acc: 0.7678 - val_loss: 8.8418 - val_acc: 0.2292\n",
            "Epoch 203/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8513 - acc: 0.7713 - val_loss: 8.8842 - val_acc: 0.2260\n",
            "Epoch 204/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8379 - acc: 0.7764 - val_loss: 8.8368 - val_acc: 0.2318\n",
            "Epoch 205/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8396 - acc: 0.7735 - val_loss: 8.8366 - val_acc: 0.2240\n",
            "Epoch 206/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8358 - acc: 0.7729 - val_loss: 8.9626 - val_acc: 0.2227\n",
            "Epoch 207/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8341 - acc: 0.7754 - val_loss: 8.8956 - val_acc: 0.2234\n",
            "Epoch 208/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8247 - acc: 0.7777 - val_loss: 8.9478 - val_acc: 0.2247\n",
            "Epoch 209/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8236 - acc: 0.7777 - val_loss: 8.8864 - val_acc: 0.2188\n",
            "Epoch 210/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8305 - acc: 0.7754 - val_loss: 8.9327 - val_acc: 0.2175\n",
            "Epoch 211/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8241 - acc: 0.7754 - val_loss: 8.9340 - val_acc: 0.2299\n",
            "Epoch 212/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8209 - acc: 0.7774 - val_loss: 8.8827 - val_acc: 0.2247\n",
            "Epoch 213/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8226 - acc: 0.7758 - val_loss: 8.9574 - val_acc: 0.2260\n",
            "Epoch 214/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8302 - acc: 0.7748 - val_loss: 8.9356 - val_acc: 0.2292\n",
            "Epoch 215/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8229 - acc: 0.7742 - val_loss: 8.9693 - val_acc: 0.2240\n",
            "Epoch 216/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8190 - acc: 0.7821 - val_loss: 8.9578 - val_acc: 0.2221\n",
            "Epoch 217/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8166 - acc: 0.7828 - val_loss: 8.9736 - val_acc: 0.2292\n",
            "Epoch 218/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8098 - acc: 0.7796 - val_loss: 8.9094 - val_acc: 0.2273\n",
            "Epoch 219/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8131 - acc: 0.7821 - val_loss: 8.9913 - val_acc: 0.2260\n",
            "Epoch 220/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8097 - acc: 0.7806 - val_loss: 8.9692 - val_acc: 0.2208\n",
            "Epoch 221/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8178 - acc: 0.7745 - val_loss: 8.9773 - val_acc: 0.2253\n",
            "Epoch 222/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8245 - acc: 0.7825 - val_loss: 9.0351 - val_acc: 0.2260\n",
            "Epoch 223/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8174 - acc: 0.7847 - val_loss: 8.9755 - val_acc: 0.2247\n",
            "Epoch 224/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8073 - acc: 0.7828 - val_loss: 8.9696 - val_acc: 0.2266\n",
            "Epoch 225/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8281 - acc: 0.7783 - val_loss: 8.9552 - val_acc: 0.2292\n",
            "Epoch 226/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8042 - acc: 0.7860 - val_loss: 9.0312 - val_acc: 0.2240\n",
            "Epoch 227/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8082 - acc: 0.7828 - val_loss: 8.9694 - val_acc: 0.2214\n",
            "Epoch 228/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.8044 - acc: 0.7834 - val_loss: 9.0397 - val_acc: 0.2188\n",
            "Epoch 229/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8031 - acc: 0.7879 - val_loss: 9.0390 - val_acc: 0.2253\n",
            "Epoch 230/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8028 - acc: 0.7821 - val_loss: 9.0016 - val_acc: 0.2234\n",
            "Epoch 231/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8013 - acc: 0.7866 - val_loss: 9.0103 - val_acc: 0.2273\n",
            "Epoch 232/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7897 - acc: 0.7853 - val_loss: 9.0558 - val_acc: 0.2266\n",
            "Epoch 233/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7899 - acc: 0.7962 - val_loss: 9.0278 - val_acc: 0.2260\n",
            "Epoch 234/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.8012 - acc: 0.7882 - val_loss: 9.0747 - val_acc: 0.2201\n",
            "Epoch 235/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7886 - acc: 0.7895 - val_loss: 9.0476 - val_acc: 0.2279\n",
            "Epoch 236/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7898 - acc: 0.7911 - val_loss: 9.0539 - val_acc: 0.2286\n",
            "Epoch 237/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7920 - acc: 0.7831 - val_loss: 9.0204 - val_acc: 0.2273\n",
            "Epoch 238/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7813 - acc: 0.7901 - val_loss: 9.0408 - val_acc: 0.2221\n",
            "Epoch 239/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7858 - acc: 0.7898 - val_loss: 9.0293 - val_acc: 0.2305\n",
            "Epoch 240/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7869 - acc: 0.7895 - val_loss: 9.0738 - val_acc: 0.2286\n",
            "Epoch 241/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7804 - acc: 0.7949 - val_loss: 9.0480 - val_acc: 0.2305\n",
            "Epoch 242/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7924 - acc: 0.7927 - val_loss: 9.0505 - val_acc: 0.2305\n",
            "Epoch 243/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7862 - acc: 0.7911 - val_loss: 9.0847 - val_acc: 0.2273\n",
            "Epoch 244/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7688 - acc: 0.7905 - val_loss: 9.0874 - val_acc: 0.2240\n",
            "Epoch 245/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7815 - acc: 0.7863 - val_loss: 9.1144 - val_acc: 0.2234\n",
            "Epoch 246/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7557 - acc: 0.8004 - val_loss: 9.1210 - val_acc: 0.2299\n",
            "Epoch 247/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7883 - acc: 0.7847 - val_loss: 9.1473 - val_acc: 0.2221\n",
            "Epoch 248/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7792 - acc: 0.7866 - val_loss: 9.0689 - val_acc: 0.2286\n",
            "Epoch 249/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7780 - acc: 0.7911 - val_loss: 9.0463 - val_acc: 0.2318\n",
            "Epoch 250/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7736 - acc: 0.7876 - val_loss: 9.0830 - val_acc: 0.2247\n",
            "Epoch 251/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7774 - acc: 0.7898 - val_loss: 9.0953 - val_acc: 0.2247\n",
            "Epoch 252/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7750 - acc: 0.7863 - val_loss: 9.1080 - val_acc: 0.2260\n",
            "Epoch 253/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7701 - acc: 0.7994 - val_loss: 9.0826 - val_acc: 0.2299\n",
            "Epoch 254/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7606 - acc: 0.7962 - val_loss: 9.1056 - val_acc: 0.2240\n",
            "Epoch 255/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7733 - acc: 0.7946 - val_loss: 9.0987 - val_acc: 0.2260\n",
            "Epoch 256/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7726 - acc: 0.7953 - val_loss: 9.1528 - val_acc: 0.2240\n",
            "Epoch 257/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7799 - acc: 0.7905 - val_loss: 9.1205 - val_acc: 0.2234\n",
            "Epoch 258/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7747 - acc: 0.7940 - val_loss: 9.1356 - val_acc: 0.2201\n",
            "Epoch 259/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7616 - acc: 0.7956 - val_loss: 9.1312 - val_acc: 0.2234\n",
            "Epoch 260/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7643 - acc: 0.7930 - val_loss: 9.1084 - val_acc: 0.2253\n",
            "Epoch 261/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7565 - acc: 0.7959 - val_loss: 9.1405 - val_acc: 0.2240\n",
            "Epoch 262/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7590 - acc: 0.7943 - val_loss: 9.1955 - val_acc: 0.2208\n",
            "Epoch 263/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7686 - acc: 0.7946 - val_loss: 9.1512 - val_acc: 0.2221\n",
            "Epoch 264/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7560 - acc: 0.7985 - val_loss: 9.1587 - val_acc: 0.2195\n",
            "Epoch 265/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7573 - acc: 0.7988 - val_loss: 9.1693 - val_acc: 0.2240\n",
            "Epoch 266/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7664 - acc: 0.7937 - val_loss: 9.1788 - val_acc: 0.2182\n",
            "Epoch 267/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7597 - acc: 0.7997 - val_loss: 9.1892 - val_acc: 0.2299\n",
            "Epoch 268/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7586 - acc: 0.8007 - val_loss: 9.1293 - val_acc: 0.2273\n",
            "Epoch 269/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7567 - acc: 0.7985 - val_loss: 9.1634 - val_acc: 0.2279\n",
            "Epoch 270/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7556 - acc: 0.7972 - val_loss: 9.1239 - val_acc: 0.2253\n",
            "Epoch 271/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7639 - acc: 0.7953 - val_loss: 9.1760 - val_acc: 0.2162\n",
            "Epoch 272/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7516 - acc: 0.8007 - val_loss: 9.1822 - val_acc: 0.2221\n",
            "Epoch 273/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7538 - acc: 0.7959 - val_loss: 9.0776 - val_acc: 0.2253\n",
            "Epoch 274/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7463 - acc: 0.8007 - val_loss: 9.1971 - val_acc: 0.2188\n",
            "Epoch 275/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7511 - acc: 0.7978 - val_loss: 9.2145 - val_acc: 0.2253\n",
            "Epoch 276/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7351 - acc: 0.8033 - val_loss: 9.2347 - val_acc: 0.2175\n",
            "Epoch 277/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7318 - acc: 0.8017 - val_loss: 9.2008 - val_acc: 0.2247\n",
            "Epoch 278/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7426 - acc: 0.8033 - val_loss: 9.1651 - val_acc: 0.2221\n",
            "Epoch 279/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7379 - acc: 0.8033 - val_loss: 9.2333 - val_acc: 0.2149\n",
            "Epoch 280/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7449 - acc: 0.7972 - val_loss: 9.1547 - val_acc: 0.2201\n",
            "Epoch 281/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7308 - acc: 0.8084 - val_loss: 9.2590 - val_acc: 0.2117\n",
            "Epoch 282/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7425 - acc: 0.8017 - val_loss: 9.2502 - val_acc: 0.2078\n",
            "Epoch 283/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7423 - acc: 0.8001 - val_loss: 9.1937 - val_acc: 0.2273\n",
            "Epoch 284/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7376 - acc: 0.7994 - val_loss: 9.1901 - val_acc: 0.2273\n",
            "Epoch 285/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7310 - acc: 0.8042 - val_loss: 9.1709 - val_acc: 0.2221\n",
            "Epoch 286/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7240 - acc: 0.8049 - val_loss: 9.2317 - val_acc: 0.2227\n",
            "Epoch 287/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7295 - acc: 0.8020 - val_loss: 9.2560 - val_acc: 0.2234\n",
            "Epoch 288/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7374 - acc: 0.8017 - val_loss: 9.2628 - val_acc: 0.2195\n",
            "Epoch 289/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7333 - acc: 0.8017 - val_loss: 9.3246 - val_acc: 0.2104\n",
            "Epoch 290/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7388 - acc: 0.8007 - val_loss: 9.3429 - val_acc: 0.2221\n",
            "Epoch 291/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7178 - acc: 0.8081 - val_loss: 9.3333 - val_acc: 0.2234\n",
            "Epoch 292/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7403 - acc: 0.8042 - val_loss: 9.2678 - val_acc: 0.2286\n",
            "Epoch 293/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7281 - acc: 0.8065 - val_loss: 9.3274 - val_acc: 0.2286\n",
            "Epoch 294/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7311 - acc: 0.8039 - val_loss: 9.2962 - val_acc: 0.2247\n",
            "Epoch 295/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7182 - acc: 0.8058 - val_loss: 9.3393 - val_acc: 0.2136\n",
            "Epoch 296/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7317 - acc: 0.8042 - val_loss: 9.3534 - val_acc: 0.2143\n",
            "Epoch 297/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7334 - acc: 0.8081 - val_loss: 9.3261 - val_acc: 0.2175\n",
            "Epoch 298/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7388 - acc: 0.8001 - val_loss: 9.3120 - val_acc: 0.2214\n",
            "Epoch 299/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7299 - acc: 0.8074 - val_loss: 9.3758 - val_acc: 0.2175\n",
            "Epoch 300/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7151 - acc: 0.8065 - val_loss: 9.3735 - val_acc: 0.2143\n",
            "Epoch 301/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7453 - acc: 0.8017 - val_loss: 9.3503 - val_acc: 0.2292\n",
            "Epoch 302/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7200 - acc: 0.8010 - val_loss: 9.3217 - val_acc: 0.2260\n",
            "Epoch 303/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7300 - acc: 0.8023 - val_loss: 9.3741 - val_acc: 0.2234\n",
            "Epoch 304/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7306 - acc: 0.8010 - val_loss: 9.3530 - val_acc: 0.2182\n",
            "Epoch 305/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7201 - acc: 0.8036 - val_loss: 9.4135 - val_acc: 0.2260\n",
            "Epoch 306/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7276 - acc: 0.8033 - val_loss: 9.3722 - val_acc: 0.2175\n",
            "Epoch 307/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7224 - acc: 0.8033 - val_loss: 9.3379 - val_acc: 0.2221\n",
            "Epoch 308/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7330 - acc: 0.8052 - val_loss: 9.4349 - val_acc: 0.2169\n",
            "Epoch 309/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7256 - acc: 0.8039 - val_loss: 9.3793 - val_acc: 0.2221\n",
            "Epoch 310/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7114 - acc: 0.8077 - val_loss: 9.4108 - val_acc: 0.2253\n",
            "Epoch 311/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7201 - acc: 0.8055 - val_loss: 9.4148 - val_acc: 0.2253\n",
            "Epoch 312/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7142 - acc: 0.8087 - val_loss: 9.2737 - val_acc: 0.2227\n",
            "Epoch 313/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7119 - acc: 0.8045 - val_loss: 9.4083 - val_acc: 0.2266\n",
            "Epoch 314/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7155 - acc: 0.8029 - val_loss: 9.4030 - val_acc: 0.2201\n",
            "Epoch 315/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7120 - acc: 0.8106 - val_loss: 9.3925 - val_acc: 0.2234\n",
            "Epoch 316/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7128 - acc: 0.8109 - val_loss: 9.4616 - val_acc: 0.2195\n",
            "Epoch 317/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7153 - acc: 0.8071 - val_loss: 9.3938 - val_acc: 0.2201\n",
            "Epoch 318/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7200 - acc: 0.8052 - val_loss: 9.3953 - val_acc: 0.2162\n",
            "Epoch 319/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7164 - acc: 0.8029 - val_loss: 9.3970 - val_acc: 0.2175\n",
            "Epoch 320/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7116 - acc: 0.8090 - val_loss: 9.4477 - val_acc: 0.2227\n",
            "Epoch 321/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7135 - acc: 0.8045 - val_loss: 9.5263 - val_acc: 0.2136\n",
            "Epoch 322/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7080 - acc: 0.8097 - val_loss: 9.5003 - val_acc: 0.2169\n",
            "Epoch 323/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7094 - acc: 0.8093 - val_loss: 9.4516 - val_acc: 0.2143\n",
            "Epoch 324/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7373 - acc: 0.7994 - val_loss: 9.4588 - val_acc: 0.2143\n",
            "Epoch 325/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7000 - acc: 0.8058 - val_loss: 9.4840 - val_acc: 0.2247\n",
            "Epoch 326/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7058 - acc: 0.8103 - val_loss: 9.5178 - val_acc: 0.2071\n",
            "Epoch 327/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7106 - acc: 0.8074 - val_loss: 9.5140 - val_acc: 0.2234\n",
            "Epoch 328/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7057 - acc: 0.8081 - val_loss: 9.5013 - val_acc: 0.2188\n",
            "Epoch 329/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7097 - acc: 0.8084 - val_loss: 9.5549 - val_acc: 0.2149\n",
            "Epoch 330/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7117 - acc: 0.8042 - val_loss: 9.4004 - val_acc: 0.2149\n",
            "Epoch 331/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7087 - acc: 0.8065 - val_loss: 9.5444 - val_acc: 0.2175\n",
            "Epoch 332/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6987 - acc: 0.8071 - val_loss: 9.5148 - val_acc: 0.2234\n",
            "Epoch 333/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7184 - acc: 0.8042 - val_loss: 9.4745 - val_acc: 0.2201\n",
            "Epoch 334/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7137 - acc: 0.8081 - val_loss: 9.4796 - val_acc: 0.2195\n",
            "Epoch 335/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7075 - acc: 0.8103 - val_loss: 9.4664 - val_acc: 0.2195\n",
            "Epoch 336/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7079 - acc: 0.8087 - val_loss: 9.5014 - val_acc: 0.2221\n",
            "Epoch 337/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7035 - acc: 0.8138 - val_loss: 9.4807 - val_acc: 0.2214\n",
            "Epoch 338/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7030 - acc: 0.8113 - val_loss: 9.5526 - val_acc: 0.2253\n",
            "Epoch 339/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6878 - acc: 0.8167 - val_loss: 9.4685 - val_acc: 0.2227\n",
            "Epoch 340/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7222 - acc: 0.8029 - val_loss: 9.5326 - val_acc: 0.2253\n",
            "Epoch 341/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7002 - acc: 0.8109 - val_loss: 9.4540 - val_acc: 0.2208\n",
            "Epoch 342/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6987 - acc: 0.8141 - val_loss: 9.4845 - val_acc: 0.2305\n",
            "Epoch 343/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7055 - acc: 0.8106 - val_loss: 9.5007 - val_acc: 0.2227\n",
            "Epoch 344/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6952 - acc: 0.8103 - val_loss: 9.5569 - val_acc: 0.2175\n",
            "Epoch 345/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7043 - acc: 0.8106 - val_loss: 9.5353 - val_acc: 0.2247\n",
            "Epoch 346/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6973 - acc: 0.8103 - val_loss: 9.5142 - val_acc: 0.2247\n",
            "Epoch 347/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6925 - acc: 0.8122 - val_loss: 9.5470 - val_acc: 0.2273\n",
            "Epoch 348/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6955 - acc: 0.8100 - val_loss: 9.5513 - val_acc: 0.2260\n",
            "Epoch 349/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7172 - acc: 0.8081 - val_loss: 9.5344 - val_acc: 0.2227\n",
            "Epoch 350/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6905 - acc: 0.8116 - val_loss: 9.5651 - val_acc: 0.2240\n",
            "Epoch 351/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7030 - acc: 0.8055 - val_loss: 9.5953 - val_acc: 0.2221\n",
            "Epoch 352/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6927 - acc: 0.8084 - val_loss: 9.5731 - val_acc: 0.2253\n",
            "Epoch 353/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6944 - acc: 0.8090 - val_loss: 9.5882 - val_acc: 0.2221\n",
            "Epoch 354/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7022 - acc: 0.8093 - val_loss: 9.5894 - val_acc: 0.2208\n",
            "Epoch 355/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6872 - acc: 0.8141 - val_loss: 9.6051 - val_acc: 0.2208\n",
            "Epoch 356/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6986 - acc: 0.8129 - val_loss: 9.5887 - val_acc: 0.2201\n",
            "Epoch 357/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6853 - acc: 0.8141 - val_loss: 9.6329 - val_acc: 0.2143\n",
            "Epoch 358/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6912 - acc: 0.8122 - val_loss: 9.5923 - val_acc: 0.2214\n",
            "Epoch 359/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6895 - acc: 0.8093 - val_loss: 9.6175 - val_acc: 0.2208\n",
            "Epoch 360/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6914 - acc: 0.8113 - val_loss: 9.5604 - val_acc: 0.2240\n",
            "Epoch 361/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.7013 - acc: 0.8109 - val_loss: 9.6048 - val_acc: 0.2195\n",
            "Epoch 362/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6963 - acc: 0.8135 - val_loss: 9.5488 - val_acc: 0.2240\n",
            "Epoch 363/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6912 - acc: 0.8122 - val_loss: 9.5723 - val_acc: 0.2162\n",
            "Epoch 364/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6974 - acc: 0.8103 - val_loss: 9.6186 - val_acc: 0.2240\n",
            "Epoch 365/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.7014 - acc: 0.8084 - val_loss: 9.5812 - val_acc: 0.2201\n",
            "Epoch 366/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6836 - acc: 0.8129 - val_loss: 9.6272 - val_acc: 0.2221\n",
            "Epoch 367/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6836 - acc: 0.8132 - val_loss: 9.6076 - val_acc: 0.2175\n",
            "Epoch 368/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6900 - acc: 0.8119 - val_loss: 9.5966 - val_acc: 0.2149\n",
            "Epoch 369/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6864 - acc: 0.8151 - val_loss: 9.5854 - val_acc: 0.2214\n",
            "Epoch 370/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6855 - acc: 0.8106 - val_loss: 9.5817 - val_acc: 0.2234\n",
            "Epoch 371/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6890 - acc: 0.8103 - val_loss: 9.6231 - val_acc: 0.2195\n",
            "Epoch 372/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6861 - acc: 0.8135 - val_loss: 9.6375 - val_acc: 0.2214\n",
            "Epoch 373/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6832 - acc: 0.8170 - val_loss: 9.5702 - val_acc: 0.2292\n",
            "Epoch 374/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6834 - acc: 0.8167 - val_loss: 9.6506 - val_acc: 0.2227\n",
            "Epoch 375/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6928 - acc: 0.8116 - val_loss: 9.5700 - val_acc: 0.2299\n",
            "Epoch 376/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6932 - acc: 0.8129 - val_loss: 9.6144 - val_acc: 0.2260\n",
            "Epoch 377/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6867 - acc: 0.8180 - val_loss: 9.6162 - val_acc: 0.2227\n",
            "Epoch 378/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6892 - acc: 0.8148 - val_loss: 9.5658 - val_acc: 0.2253\n",
            "Epoch 379/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6944 - acc: 0.8119 - val_loss: 9.5871 - val_acc: 0.2279\n",
            "Epoch 380/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6742 - acc: 0.8167 - val_loss: 9.5857 - val_acc: 0.2253\n",
            "Epoch 381/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6856 - acc: 0.8164 - val_loss: 9.6066 - val_acc: 0.2240\n",
            "Epoch 382/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6850 - acc: 0.8109 - val_loss: 9.6004 - val_acc: 0.2208\n",
            "Epoch 383/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6912 - acc: 0.8122 - val_loss: 9.6724 - val_acc: 0.2234\n",
            "Epoch 384/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6853 - acc: 0.8157 - val_loss: 9.6898 - val_acc: 0.2247\n",
            "Epoch 385/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6693 - acc: 0.8138 - val_loss: 9.6967 - val_acc: 0.2175\n",
            "Epoch 386/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6942 - acc: 0.8132 - val_loss: 9.7503 - val_acc: 0.2175\n",
            "Epoch 387/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6673 - acc: 0.8183 - val_loss: 9.7561 - val_acc: 0.2188\n",
            "Epoch 388/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6852 - acc: 0.8113 - val_loss: 9.7087 - val_acc: 0.2214\n",
            "Epoch 389/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6781 - acc: 0.8161 - val_loss: 9.6871 - val_acc: 0.2201\n",
            "Epoch 390/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6803 - acc: 0.8148 - val_loss: 9.6368 - val_acc: 0.2188\n",
            "Epoch 391/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6755 - acc: 0.8116 - val_loss: 9.6355 - val_acc: 0.2240\n",
            "Epoch 392/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6814 - acc: 0.8145 - val_loss: 9.5780 - val_acc: 0.2266\n",
            "Epoch 393/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6791 - acc: 0.8138 - val_loss: 9.6150 - val_acc: 0.2208\n",
            "Epoch 394/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6755 - acc: 0.8132 - val_loss: 9.6469 - val_acc: 0.2221\n",
            "Epoch 395/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6805 - acc: 0.8161 - val_loss: 9.6077 - val_acc: 0.2195\n",
            "Epoch 396/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6762 - acc: 0.8154 - val_loss: 9.6603 - val_acc: 0.2188\n",
            "Epoch 397/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6714 - acc: 0.8173 - val_loss: 9.6828 - val_acc: 0.2227\n",
            "Epoch 398/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6895 - acc: 0.8151 - val_loss: 9.7496 - val_acc: 0.2221\n",
            "Epoch 399/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6745 - acc: 0.8167 - val_loss: 9.7599 - val_acc: 0.2221\n",
            "Epoch 400/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6824 - acc: 0.8109 - val_loss: 9.6825 - val_acc: 0.2234\n",
            "Epoch 401/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6736 - acc: 0.8177 - val_loss: 9.7436 - val_acc: 0.2195\n",
            "Epoch 402/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6778 - acc: 0.8141 - val_loss: 9.7224 - val_acc: 0.2156\n",
            "Epoch 403/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6700 - acc: 0.8199 - val_loss: 9.7615 - val_acc: 0.2182\n",
            "Epoch 404/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6813 - acc: 0.8154 - val_loss: 9.6824 - val_acc: 0.2318\n",
            "Epoch 405/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6725 - acc: 0.8170 - val_loss: 9.7219 - val_acc: 0.2279\n",
            "Epoch 406/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6622 - acc: 0.8177 - val_loss: 9.7152 - val_acc: 0.2240\n",
            "Epoch 407/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6821 - acc: 0.8132 - val_loss: 9.6993 - val_acc: 0.2240\n",
            "Epoch 408/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6830 - acc: 0.8116 - val_loss: 9.7703 - val_acc: 0.2260\n",
            "Epoch 409/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6818 - acc: 0.8173 - val_loss: 9.7978 - val_acc: 0.2247\n",
            "Epoch 410/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6836 - acc: 0.8093 - val_loss: 9.7097 - val_acc: 0.2234\n",
            "Epoch 411/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6678 - acc: 0.8193 - val_loss: 9.7143 - val_acc: 0.2266\n",
            "Epoch 412/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6853 - acc: 0.8148 - val_loss: 9.8012 - val_acc: 0.2286\n",
            "Epoch 413/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6762 - acc: 0.8148 - val_loss: 9.7543 - val_acc: 0.2188\n",
            "Epoch 414/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6636 - acc: 0.8183 - val_loss: 9.7932 - val_acc: 0.2292\n",
            "Epoch 415/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6683 - acc: 0.8199 - val_loss: 9.7665 - val_acc: 0.2253\n",
            "Epoch 416/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6689 - acc: 0.8145 - val_loss: 9.7579 - val_acc: 0.2299\n",
            "Epoch 417/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6733 - acc: 0.8170 - val_loss: 9.7956 - val_acc: 0.2260\n",
            "Epoch 418/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6728 - acc: 0.8167 - val_loss: 9.7256 - val_acc: 0.2260\n",
            "Epoch 419/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6685 - acc: 0.8154 - val_loss: 9.7935 - val_acc: 0.2240\n",
            "Epoch 420/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6665 - acc: 0.8157 - val_loss: 9.8190 - val_acc: 0.2266\n",
            "Epoch 421/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6772 - acc: 0.8157 - val_loss: 9.7722 - val_acc: 0.2292\n",
            "Epoch 422/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6788 - acc: 0.8148 - val_loss: 9.7639 - val_acc: 0.2260\n",
            "Epoch 423/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6746 - acc: 0.8129 - val_loss: 9.7799 - val_acc: 0.2260\n",
            "Epoch 424/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6831 - acc: 0.8167 - val_loss: 9.7887 - val_acc: 0.2273\n",
            "Epoch 425/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6646 - acc: 0.8225 - val_loss: 9.7855 - val_acc: 0.2195\n",
            "Epoch 426/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6805 - acc: 0.8122 - val_loss: 9.7671 - val_acc: 0.2208\n",
            "Epoch 427/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6592 - acc: 0.8135 - val_loss: 9.7867 - val_acc: 0.2169\n",
            "Epoch 428/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6837 - acc: 0.8196 - val_loss: 9.8225 - val_acc: 0.2227\n",
            "Epoch 429/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6756 - acc: 0.8135 - val_loss: 9.7917 - val_acc: 0.2253\n",
            "Epoch 430/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6680 - acc: 0.8132 - val_loss: 9.7258 - val_acc: 0.2227\n",
            "Epoch 431/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6721 - acc: 0.8177 - val_loss: 9.8131 - val_acc: 0.2214\n",
            "Epoch 432/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6655 - acc: 0.8135 - val_loss: 9.8249 - val_acc: 0.2156\n",
            "Epoch 433/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6723 - acc: 0.8154 - val_loss: 9.7268 - val_acc: 0.2266\n",
            "Epoch 434/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6625 - acc: 0.8167 - val_loss: 9.7686 - val_acc: 0.2253\n",
            "Epoch 435/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6859 - acc: 0.8084 - val_loss: 9.7671 - val_acc: 0.2253\n",
            "Epoch 436/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6731 - acc: 0.8180 - val_loss: 9.7288 - val_acc: 0.2253\n",
            "Epoch 437/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6781 - acc: 0.8103 - val_loss: 9.8006 - val_acc: 0.2234\n",
            "Epoch 438/500\n",
            "3126/3126 [==============================] - 9s 3ms/step - loss: 0.6758 - acc: 0.8157 - val_loss: 9.7359 - val_acc: 0.2208\n",
            "Epoch 439/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6784 - acc: 0.8157 - val_loss: 9.8132 - val_acc: 0.2240\n",
            "Epoch 440/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6768 - acc: 0.8090 - val_loss: 9.8133 - val_acc: 0.2214\n",
            "Epoch 441/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6660 - acc: 0.8180 - val_loss: 9.7902 - val_acc: 0.2266\n",
            "Epoch 442/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6653 - acc: 0.8145 - val_loss: 9.8022 - val_acc: 0.2208\n",
            "Epoch 443/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6716 - acc: 0.8193 - val_loss: 9.8544 - val_acc: 0.2214\n",
            "Epoch 444/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6646 - acc: 0.8157 - val_loss: 9.7754 - val_acc: 0.2188\n",
            "Epoch 445/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6736 - acc: 0.8138 - val_loss: 9.8170 - val_acc: 0.2195\n",
            "Epoch 446/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6758 - acc: 0.8135 - val_loss: 9.8242 - val_acc: 0.2253\n",
            "Epoch 447/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6687 - acc: 0.8167 - val_loss: 9.8872 - val_acc: 0.2201\n",
            "Epoch 448/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6612 - acc: 0.8186 - val_loss: 9.8933 - val_acc: 0.2130\n",
            "Epoch 449/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6622 - acc: 0.8202 - val_loss: 9.8717 - val_acc: 0.2156\n",
            "Epoch 450/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6761 - acc: 0.8186 - val_loss: 9.8677 - val_acc: 0.2091\n",
            "Epoch 451/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6682 - acc: 0.8129 - val_loss: 9.8776 - val_acc: 0.2182\n",
            "Epoch 452/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6799 - acc: 0.8164 - val_loss: 9.8674 - val_acc: 0.2162\n",
            "Epoch 453/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6644 - acc: 0.8122 - val_loss: 9.7950 - val_acc: 0.2292\n",
            "Epoch 454/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6739 - acc: 0.8138 - val_loss: 9.8020 - val_acc: 0.2188\n",
            "Epoch 455/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6748 - acc: 0.8125 - val_loss: 9.8100 - val_acc: 0.2195\n",
            "Epoch 456/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6763 - acc: 0.8199 - val_loss: 9.8104 - val_acc: 0.2175\n",
            "Epoch 457/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6754 - acc: 0.8170 - val_loss: 9.8792 - val_acc: 0.2182\n",
            "Epoch 458/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6635 - acc: 0.8177 - val_loss: 9.8147 - val_acc: 0.2253\n",
            "Epoch 459/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6626 - acc: 0.8231 - val_loss: 9.8202 - val_acc: 0.2182\n",
            "Epoch 460/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6679 - acc: 0.8205 - val_loss: 9.8186 - val_acc: 0.2227\n",
            "Epoch 461/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6582 - acc: 0.8161 - val_loss: 9.8526 - val_acc: 0.2201\n",
            "Epoch 462/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6629 - acc: 0.8228 - val_loss: 9.8537 - val_acc: 0.2182\n",
            "Epoch 463/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6654 - acc: 0.8189 - val_loss: 9.8646 - val_acc: 0.2221\n",
            "Epoch 464/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6770 - acc: 0.8199 - val_loss: 9.8968 - val_acc: 0.2227\n",
            "Epoch 465/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6677 - acc: 0.8170 - val_loss: 9.7853 - val_acc: 0.2188\n",
            "Epoch 466/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6823 - acc: 0.8132 - val_loss: 9.8085 - val_acc: 0.2084\n",
            "Epoch 467/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6757 - acc: 0.8180 - val_loss: 9.7525 - val_acc: 0.2130\n",
            "Epoch 468/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6776 - acc: 0.8193 - val_loss: 9.8429 - val_acc: 0.2110\n",
            "Epoch 469/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6670 - acc: 0.8186 - val_loss: 9.8428 - val_acc: 0.2169\n",
            "Epoch 470/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6774 - acc: 0.8151 - val_loss: 9.8366 - val_acc: 0.2201\n",
            "Epoch 471/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6577 - acc: 0.8196 - val_loss: 9.8890 - val_acc: 0.2195\n",
            "Epoch 472/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6725 - acc: 0.8161 - val_loss: 9.8392 - val_acc: 0.2221\n",
            "Epoch 473/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6802 - acc: 0.8132 - val_loss: 9.8487 - val_acc: 0.2201\n",
            "Epoch 474/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6615 - acc: 0.8234 - val_loss: 9.8759 - val_acc: 0.2175\n",
            "Epoch 475/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6856 - acc: 0.8167 - val_loss: 9.8012 - val_acc: 0.2247\n",
            "Epoch 476/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6613 - acc: 0.8202 - val_loss: 9.8433 - val_acc: 0.2162\n",
            "Epoch 477/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6576 - acc: 0.8228 - val_loss: 9.8496 - val_acc: 0.2149\n",
            "Epoch 478/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6739 - acc: 0.8119 - val_loss: 9.8840 - val_acc: 0.2162\n",
            "Epoch 479/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6716 - acc: 0.8135 - val_loss: 9.7757 - val_acc: 0.2195\n",
            "Epoch 480/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6703 - acc: 0.8161 - val_loss: 9.8410 - val_acc: 0.2188\n",
            "Epoch 481/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6650 - acc: 0.8212 - val_loss: 9.8881 - val_acc: 0.2136\n",
            "Epoch 482/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6652 - acc: 0.8218 - val_loss: 9.8695 - val_acc: 0.2266\n",
            "Epoch 483/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6588 - acc: 0.8167 - val_loss: 9.9017 - val_acc: 0.2221\n",
            "Epoch 484/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6651 - acc: 0.8199 - val_loss: 9.8473 - val_acc: 0.2162\n",
            "Epoch 485/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6737 - acc: 0.8161 - val_loss: 9.8122 - val_acc: 0.2175\n",
            "Epoch 486/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6718 - acc: 0.8167 - val_loss: 9.8244 - val_acc: 0.2260\n",
            "Epoch 487/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6643 - acc: 0.8177 - val_loss: 9.8645 - val_acc: 0.2221\n",
            "Epoch 488/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6732 - acc: 0.8125 - val_loss: 9.8393 - val_acc: 0.2175\n",
            "Epoch 489/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6622 - acc: 0.8196 - val_loss: 9.8824 - val_acc: 0.2221\n",
            "Epoch 490/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6709 - acc: 0.8148 - val_loss: 9.9128 - val_acc: 0.2221\n",
            "Epoch 491/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6654 - acc: 0.8167 - val_loss: 9.9815 - val_acc: 0.2117\n",
            "Epoch 492/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6621 - acc: 0.8193 - val_loss: 9.8968 - val_acc: 0.2227\n",
            "Epoch 493/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6639 - acc: 0.8157 - val_loss: 9.8971 - val_acc: 0.2149\n",
            "Epoch 494/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6764 - acc: 0.8161 - val_loss: 9.9085 - val_acc: 0.2182\n",
            "Epoch 495/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6682 - acc: 0.8199 - val_loss: 9.8686 - val_acc: 0.2221\n",
            "Epoch 496/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6718 - acc: 0.8205 - val_loss: 9.9437 - val_acc: 0.2299\n",
            "Epoch 497/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6717 - acc: 0.8186 - val_loss: 9.9065 - val_acc: 0.2182\n",
            "Epoch 498/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6752 - acc: 0.8177 - val_loss: 9.9683 - val_acc: 0.2253\n",
            "Epoch 499/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6659 - acc: 0.8209 - val_loss: 9.9237 - val_acc: 0.2162\n",
            "Epoch 500/500\n",
            "3126/3126 [==============================] - 10s 3ms/step - loss: 0.6648 - acc: 0.8237 - val_loss: 9.8733 - val_acc: 0.2162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV81w3hSxifS",
        "colab_type": "text"
      },
      "source": [
        "This is the prefered model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGnfoWKPHdlS",
        "colab_type": "code",
        "outputId": "caf68fee-8aad-4e65-999c-5abe8e36839b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "SVG(model_to_dot(model3,show_shapes= True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 351.00 470.00\" width=\"292pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 347,-466 347,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139714059169576 -->\n<g class=\"node\" id=\"node1\">\n<title>139714059169576</title>\n<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 343,-461.5 343,-415.5 0,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-434.8\">embedding_3_input: InputLayer</text>\n<polyline fill=\"none\" points=\"205,-415.5 205,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"205,-438.5 263,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"263,-415.5 263,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-446.3\">(None, 21)</text>\n<polyline fill=\"none\" points=\"263,-438.5 343,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-423.3\">(None, 21)</text>\n</g>\n<!-- 139714059169688 -->\n<g class=\"node\" id=\"node2\">\n<title>139714059169688</title>\n<polygon fill=\"none\" points=\"6,-332.5 6,-378.5 337,-378.5 337,-332.5 6,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91.5\" y=\"-351.8\">embedding_3: Embedding</text>\n<polyline fill=\"none\" points=\"177,-332.5 177,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"177,-355.5 235,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"235,-332.5 235,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-363.3\">(None, 21)</text>\n<polyline fill=\"none\" points=\"235,-355.5 337,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-340.3\">(None, 21, 21)</text>\n</g>\n<!-- 139714059169576&#45;&gt;139714059169688 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139714059169576-&gt;139714059169688</title>\n<path d=\"M171.5,-415.3799C171.5,-407.1745 171.5,-397.7679 171.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-388.784 171.5,-378.784 168.0001,-388.784 175.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139714059321472 -->\n<g class=\"node\" id=\"node3\">\n<title>139714059321472</title>\n<polygon fill=\"none\" points=\"36.5,-249.5 36.5,-295.5 306.5,-295.5 306.5,-249.5 36.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-268.8\">lstm_5: LSTM</text>\n<polyline fill=\"none\" points=\"138.5,-249.5 138.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"138.5,-272.5 196.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"196.5,-249.5 196.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-280.3\">(None, 21, 21)</text>\n<polyline fill=\"none\" points=\"196.5,-272.5 306.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-257.3\">(None, 21, 141)</text>\n</g>\n<!-- 139714059169688&#45;&gt;139714059321472 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139714059169688-&gt;139714059321472</title>\n<path d=\"M171.5,-332.3799C171.5,-324.1745 171.5,-314.7679 171.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-305.784 171.5,-295.784 168.0001,-305.784 175.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139714059137264 -->\n<g class=\"node\" id=\"node4\">\n<title>139714059137264</title>\n<polygon fill=\"none\" points=\"20.5,-166.5 20.5,-212.5 322.5,-212.5 322.5,-166.5 20.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-185.8\">dropout_1: Dropout</text>\n<polyline fill=\"none\" points=\"154.5,-166.5 154.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"154.5,-189.5 212.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"212.5,-166.5 212.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-197.3\">(None, 21, 141)</text>\n<polyline fill=\"none\" points=\"212.5,-189.5 322.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-174.3\">(None, 21, 141)</text>\n</g>\n<!-- 139714059321472&#45;&gt;139714059137264 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139714059321472-&gt;139714059137264</title>\n<path d=\"M171.5,-249.3799C171.5,-241.1745 171.5,-231.7679 171.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-222.784 171.5,-212.784 168.0001,-222.784 175.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139714059324440 -->\n<g class=\"node\" id=\"node5\">\n<title>139714059324440</title>\n<polygon fill=\"none\" points=\"36.5,-83.5 36.5,-129.5 306.5,-129.5 306.5,-83.5 36.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-102.8\">lstm_6: LSTM</text>\n<polyline fill=\"none\" points=\"138.5,-83.5 138.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"138.5,-106.5 196.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"196.5,-83.5 196.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-114.3\">(None, 21, 141)</text>\n<polyline fill=\"none\" points=\"196.5,-106.5 306.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-91.3\">(None, 100)</text>\n</g>\n<!-- 139714059137264&#45;&gt;139714059324440 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139714059137264-&gt;139714059324440</title>\n<path d=\"M171.5,-166.3799C171.5,-158.1745 171.5,-148.7679 171.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-139.784 171.5,-129.784 168.0001,-139.784 175.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139714058039080 -->\n<g class=\"node\" id=\"node6\">\n<title>139714058039080</title>\n<polygon fill=\"none\" points=\"45.5,-.5 45.5,-46.5 297.5,-46.5 297.5,-.5 45.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99\" y=\"-19.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"152.5,-.5 152.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"152.5,-23.5 210.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"210.5,-.5 210.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-31.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"210.5,-23.5 297.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-8.3\">(None, 893)</text>\n</g>\n<!-- 139714059324440&#45;&gt;139714058039080 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139714059324440-&gt;139714058039080</title>\n<path d=\"M171.5,-83.3799C171.5,-75.1745 171.5,-65.7679 171.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-56.784 171.5,-46.784 168.0001,-56.784 175.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QUKjhzOq-E7",
        "colab_type": "code",
        "outputId": "77324599-1f35-4969-807b-cd904a09ca0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "SVG(model_to_dot(model,show_shapes= True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 351.00 470.00\" width=\"292pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 347,-466 347,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140427481319968 -->\n<g class=\"node\" id=\"node1\">\n<title>140427481319968</title>\n<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 343,-461.5 343,-415.5 0,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.5\" y=\"-434.8\">embedding_5_input: InputLayer</text>\n<polyline fill=\"none\" points=\"205,-415.5 205,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"205,-438.5 263,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"263,-415.5 263,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-446.3\">(None, 21)</text>\n<polyline fill=\"none\" points=\"263,-438.5 343,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-423.3\">(None, 21)</text>\n</g>\n<!-- 140427481365248 -->\n<g class=\"node\" id=\"node2\">\n<title>140427481365248</title>\n<polygon fill=\"none\" points=\"6,-332.5 6,-378.5 337,-378.5 337,-332.5 6,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91.5\" y=\"-351.8\">embedding_5: Embedding</text>\n<polyline fill=\"none\" points=\"177,-332.5 177,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"177,-355.5 235,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"235,-332.5 235,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-363.3\">(None, 21)</text>\n<polyline fill=\"none\" points=\"235,-355.5 337,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-340.3\">(None, 21, 10)</text>\n</g>\n<!-- 140427481319968&#45;&gt;140427481365248 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140427481319968-&gt;140427481365248</title>\n<path d=\"M171.5,-415.3799C171.5,-407.1745 171.5,-397.7679 171.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-388.784 171.5,-378.784 168.0001,-388.784 175.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140429160625880 -->\n<g class=\"node\" id=\"node3\">\n<title>140429160625880</title>\n<polygon fill=\"none\" points=\"36.5,-249.5 36.5,-295.5 306.5,-295.5 306.5,-249.5 36.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-268.8\">lstm_9: LSTM</text>\n<polyline fill=\"none\" points=\"138.5,-249.5 138.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"138.5,-272.5 196.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"196.5,-249.5 196.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-280.3\">(None, 21, 10)</text>\n<polyline fill=\"none\" points=\"196.5,-272.5 306.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-257.3\">(None, 21, 140)</text>\n</g>\n<!-- 140427481365248&#45;&gt;140429160625880 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140427481365248-&gt;140429160625880</title>\n<path d=\"M171.5,-332.3799C171.5,-324.1745 171.5,-314.7679 171.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-305.784 171.5,-295.784 168.0001,-305.784 175.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140427489089856 -->\n<g class=\"node\" id=\"node4\">\n<title>140427489089856</title>\n<polygon fill=\"none\" points=\"20.5,-166.5 20.5,-212.5 322.5,-212.5 322.5,-166.5 20.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-185.8\">dropout_5: Dropout</text>\n<polyline fill=\"none\" points=\"154.5,-166.5 154.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"154.5,-189.5 212.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"212.5,-166.5 212.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-197.3\">(None, 21, 140)</text>\n<polyline fill=\"none\" points=\"212.5,-189.5 322.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-174.3\">(None, 21, 140)</text>\n</g>\n<!-- 140429160625880&#45;&gt;140427489089856 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140429160625880-&gt;140427489089856</title>\n<path d=\"M171.5,-249.3799C171.5,-241.1745 171.5,-231.7679 171.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-222.784 171.5,-212.784 168.0001,-222.784 175.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140427481402392 -->\n<g class=\"node\" id=\"node5\">\n<title>140427481402392</title>\n<polygon fill=\"none\" points=\"32.5,-83.5 32.5,-129.5 310.5,-129.5 310.5,-83.5 32.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.5\" y=\"-102.8\">lstm_10: LSTM</text>\n<polyline fill=\"none\" points=\"142.5,-83.5 142.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"142.5,-106.5 200.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"171.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"200.5,-83.5 200.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-114.3\">(None, 21, 140)</text>\n<polyline fill=\"none\" points=\"200.5,-106.5 310.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-91.3\">(None, 100)</text>\n</g>\n<!-- 140427489089856&#45;&gt;140427481402392 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140427489089856-&gt;140427481402392</title>\n<path d=\"M171.5,-166.3799C171.5,-158.1745 171.5,-148.7679 171.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-139.784 171.5,-129.784 168.0001,-139.784 175.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140427481275864 -->\n<g class=\"node\" id=\"node6\">\n<title>140427481275864</title>\n<polygon fill=\"none\" points=\"45.5,-.5 45.5,-46.5 297.5,-46.5 297.5,-.5 45.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99\" y=\"-19.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"152.5,-.5 152.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"152.5,-23.5 210.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"210.5,-.5 210.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-31.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"210.5,-23.5 297.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254\" y=\"-8.3\">(None, 893)</text>\n</g>\n<!-- 140427481402392&#45;&gt;140427481275864 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140427481402392-&gt;140427481275864</title>\n<path d=\"M171.5,-83.3799C171.5,-75.1745 171.5,-65.7679 171.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"175.0001,-56.784 171.5,-46.784 168.0001,-56.784 175.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVBVabJqqOd",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Testing and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqH8ARjfydP3",
        "colab_type": "text"
      },
      "source": [
        "Finally, we need to actually make predictions and check the performance of our trained model.\n",
        "\n",
        "The simplest way to make predictions is to first start off with a seed sequence as input, generate the next word, then update the seed sequence to add the generated word on the end and predict the new sequence. This process is repeated for as long as we want to predict new words.\n",
        "\n",
        "We can pick a random input pattern as our seed text, then print generated words as we generate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AygwJlaUdFEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text, nums_of_next_words, max_sequence_len):\n",
        "  output_text = []\n",
        "  input_text = seed_text\n",
        "  for i in range(nums_of_next_words):\n",
        "    encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    pad_encoded = pad_sequences([encoded_text], maxlen=max_sequence_len-1, truncating='pre')\n",
        "    pred_word_ind = model3.predict_classes(pad_encoded,verbose=0)[0]\n",
        "        \n",
        "    pred_word = tokenizer.index_word[pred_word_ind]\n",
        "    input_text += ' '+pred_word\n",
        "    output_text.append(pred_word)\n",
        "\n",
        "\n",
        "  return seed_text+\" \"+' '.join(output_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKXlRDDPzkYc",
        "colab_type": "text"
      },
      "source": [
        "You can enter an input sequence of text to get the prediction according to the max length of sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXA8DF_ZN8-y",
        "colab_type": "code",
        "outputId": "a85edcac-27ca-4dbb-afeb-d994ec3c78d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print (generate_text(\"naughty\", 6, max_sequence_len))\n",
        "print (generate_text(\"man is slicing an\", 1, max_sequence_len))\n",
        "print (generate_text(\"cat resting\", 4, max_sequence_len))\n",
        "print (generate_text(\"man riding skateboard\", 3, max_sequence_len))\n",
        "print (generate_text(\"woman frying\", 3, max_sequence_len))\n",
        "print (generate_text(\"black dog\", 10, max_sequence_len))\n",
        "print (generate_text(\"dancingnd three kids\", 5, max_sequence_len))\n",
        "print (generate_text(\"making sketch\", 5, max_sequence_len))\n",
        "print (generate_text(\"man\", 5, max_sequence_len))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naughty is driving diced an air in\n",
            "man is slicing an hair\n",
            "cat resting people are driving four\n",
            "man riding skateboard is man is\n",
            "woman frying people are driving\n",
            "black dog are driving four air in a pot is rocky paperwork\n",
            "dancingnd three kids are eating the food and\n",
            "making sketch people are ignoring the nearby\n",
            "man is typing on a keyboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6TUOAtlkzVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.save('lstm_text_gen_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJIbEvvgejOq",
        "colab_type": "code",
        "outputId": "a3f529c9-dfc0-4af8-b12f-4218a295ec28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print (generate_text(\"naughty\", 6, max_sequence_len))\n",
        "print (generate_text(\"man is slicing an\", 1, max_sequence_len))\n",
        "print (generate_text(\"cat resting\", 4, max_sequence_len))\n",
        "print (generate_text(\"man riding skateboard\", 3, max_sequence_len))\n",
        "print (generate_text(\"woman frying\", 3, max_sequence_len))\n",
        "print (generate_text(\"black dog\", 10, max_sequence_len))\n",
        "print (generate_text(\"dancingnd three kids\", 5, max_sequence_len))\n",
        "print (generate_text(\"making sketch\", 5, max_sequence_len))\n",
        "print (generate_text(\"man\", 5, max_sequence_len))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naughty men are being man in a\n",
            "man is slicing an onion\n",
            "cat resting a man is playing\n",
            "man riding skateboard is standing in\n",
            "woman frying no man in\n",
            "black dog is not playing a guitar and a man is standing\n",
            "dancingnd three kids are being man in a\n",
            "making sketch a man is no man\n",
            "man is no man in a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hfDJYtxl_YL",
        "colab_type": "text"
      },
      "source": [
        "Maybe you will face with the lots of repetitive words like $\"man, is, playing, a, ...\"$(depends on the number of epochs). \n",
        "So we need to encounter the problem of repetitive words and make the generated sentences meaningful. One simple way is to remove some of them.\n",
        "\n",
        "In the next cells, remove some of the repetitive words from your data, then train the model. Compare the outputs with the previous model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HLRqAXJhC6u",
        "colab_type": "code",
        "outputId": "5eea45cf-e78e-46c4-ef2b-8a20807353bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# remove some words\n",
        "delete_list = [\"A \", \" is\", \" a\", \"water\", \"playing\"]\n",
        "\n",
        "infile = \"EngSentences.txt\"\n",
        "myfile = open(infile).read()\n",
        "for w in delete_list:\n",
        "  myfile = myfile.replace(w, '')\n",
        "outfile = \"mycleandata.txt\"\n",
        "f = open(outfile,'w')\n",
        "f.write(myfile)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTJF2m-r9lKc",
        "colab_type": "text"
      },
      "source": [
        "Train the model again and see the outputs. In order to have better result, you can change the structure and hyperparameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhNTrmgwct6q",
        "colab_type": "code",
        "outputId": "657842fd-7647-44ba-c88a-fefa2780281e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!./src/run_dataprep.sh data/train.json data/dev.json data/test.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: ./src/run_dataprep.sh: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcP-Z7Sa_mfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model2(inputs, labels, max_sequence_len, all_words):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(all_words, max_sequence_len-1,input_length=max_sequence_len-1))\n",
        "  model.add(LSTM(256))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(all_words,activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  history = model.fit(inputs,labels,batch_size=32,validation_split=0.33,nb_epoch=500,verbose=1)\n",
        "\n",
        "  return model, history\n",
        "\n",
        "def generate_text2(seed_text, nums_of_next_words, max_sequence_len):\n",
        "  output_text = []\n",
        "  input_text = seed_text\n",
        "  for i in range(nums_of_next_words):\n",
        "    encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    pad_encoded = pad_sequences([encoded_text], maxlen=max_sequence_len-1, truncating='pre')\n",
        "    pred_word_ind = model4.predict_classes(pad_encoded,verbose=0)[0]\n",
        "        \n",
        "    pred_word = tokenizer.index_word[pred_word_ind]\n",
        "    input_text += ' '+pred_word\n",
        "    output_text.append(pred_word)\n",
        "\n",
        "\n",
        "  return seed_text+\" \"+' '.join(output_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jhCMzg09iGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('mycleandata.txt').read()\n",
        "inputs, labels, max_sequence_len, all_words = dataset_preparation(data)\n",
        "model4, history4 = create_model2(inputs, labels, max_sequence_len, all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYParZga-fVw",
        "colab_type": "code",
        "outputId": "269b3430-de1f-436e-eea4-d53612ef7dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print (generate_text2(\"man is slicing an\", 1, max_sequence_len))\n",
        "print (generate_text2(\"cat resting\", 4, max_sequence_len))\n",
        "print (generate_text2(\"man riding skateboard\", 3, max_sequence_len))\n",
        "print (generate_text2(\"woman frying\", 3, max_sequence_len))\n",
        "print (generate_text2(\"black dog\", 10, max_sequence_len))\n",
        "print (generate_text2(\"dancing and three kids\", 5, max_sequence_len))\n",
        "print (generate_text2(\"making sketch\", 5, max_sequence_len))\n",
        "print (generate_text2(\"man\", 5, max_sequence_len))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man is slicing an steak\n",
            "cat resting on chair which board\n",
            "man riding skateboard in park in\n",
            "woman frying chicken from frying\n",
            "black dog sitting in the grassnd keeping its mouth closed closed road\n",
            "dancing and three kids into the beach pool that\n",
            "making sketch racing in lake ice field\n",
            "man in car pulling up beside\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRclM8RJEYR1",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we can define different approaches, but there is still a lot of room for improvement. explain various ideas that may further improve the model that you could experiment.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uQi_nEPFLW_",
        "colab_type": "text"
      },
      "source": [
        "**we can use attention for improvement! and bigger data also is good choice**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7kb7BZ8zXyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}